#(c) 2012 Massachusetts Institute of Technology. All Rights Reserved
# Code written by: Maksim Imakaev (imakaev@mit.edu)


"""
This is a module class for fragment-level Hi-C data analysis.
The base class "HiCdataset" can load, save and merge Hi-C datasets,
perform certain filters, and save binned heatmaps.

Additional class HiCStatistics contains methods to analyze HiC data
on a fragment level.
This includes read statistics, scalings, etc.

Input data
----------

When used together with iterative mapping, this class can load
files from h5dicts created by iterative mapping.

This method can also input any dictionary-like structure, such as a dictionary,
np.savez, etc. The minimal subset of information are positions of two reads,
but providing strand informations is adviced.

If restriction fragment assignment is not provided,
it will be automatically recalculated.

.. warning ::
    1-bp difference in positions of restriction sites will force certain
    algorithms, such as scaling calculations, to throw an exception. It is
    adviced to supply restriction site data only if it was generated by
    iterative mapping code.

Concepts
--------

All read data is stored in a synchronized h5dict-based dictionary of arrays.
Each variable has a fixed name and type, as specified in the self.vectors
variable. Whenever the variable is accessed from the program, it is loaded from
the h5dict.

All fragment data is stored in RAM, and can be re-created at any point using
:py:func:`rebuildFragments <HiCdataset.rebuildFragments>`

Whenever a set of reads needs to be excluded from the dataset, a
:py:func:`maskFilter  <HiCdataset.maskFilter>` method is called,
that goes over all datasets and overrides them.
This method automatically rebuilds fragments.

Filtering the data
------------------

This class has many build-in methods for filtering the data.
However, one can easily construct another filter as presented in
multiple one-liner examples below

.. code-block:: python

    >>> Dset = HiCdataset(**kwargs)
    >>> Dset.fragmentFilter((Dset.ufragmentlen >1000) * \
    (Dset.ufragmentlen < 4000))
    #Keep reads from fragments between 1kb and 4kb long.

    >>> Dset.maskFilter(Dset.DS)   #Keep only DS reads
    >>> Dset.maskFilter(Dset.chrms1 == Dset.chrms2)  #keep only cis reads

    >>> Dset.maskFilter((Dset.chrms1 !=14) + (Dset.chrms2 !=14))
    #Exclude all reads from chromosome 15 (yes, chromosomes are zero-based!)

    >>> Dset.maskFilter(Dset.dist1 + Dset.dist2 > 500)
    #Keep only random breaks, if 500 is maximum molecule length

-------------------------------------------------------------------------------

API documentation
-----------------



"""

import warnings
import os
from copy import copy
from mirnylib.genome import Genome
import hiclib.mapping
import numpy as np
from numpy import array as na
from scipy import stats
import matplotlib.pyplot as plt
from mirnylib.h5dict import h5dict
from mirnylib.plotting import mat_img
from mirnylib import numutils
from mirnylib.numutils import arrayInArray, sumByArray, \
    uniqueIndex, chunkedUnique, fasterBooleanIndexing, fillDiagonal

r_ = np.r_

def corr(x, y):
    return stats.spearmanr(x, y)[0]

class HiCdataset(object):
    """Base class to operate on HiC dataset.

    This class stores all information about HiC reads on a hard drive.
    Whenever a variable corresponding to any record is used,
    it is loaded/saved from/to the HDD.

    If you apply any filters to a dataset, it will actually modify
    the content of the current working file.
    Thus, to preserve the data, loading datasets is advised. """

    def __init__(self, filename, genome, maximumMoleculeLength=500,
                 override="deprecated", autoFlush="deprecated",
                 inMemory=False, mode="a"):
        """
        __init__ method

        Initializes empty dataset by default.
        If "override" is False, works with existing dataset.

        Parameters
        ----------
        filename : string
            A filename to store HiC dataset in an HDF5 file.
        genome : folder with genome, or Genome object
            A folder with fastq files of the genome
            and gap table from Genome browser.
            Alternatively, mirnylib.genome.Genome object.
        maximumMoleculeLength : int, optional
            Maximum length of molecules in the HiC library,
            used as a cutoff for dangling ends filter
        override : bool, optional, deprecated
            If true, file will be overwritten. Deprecated,
            use "mode = 'w'" instead of "override = True".
        autoFlush : bool, optional, deprecated
            Set to True to disable autoflush -
            possibly speeds up read/write operations.
            Currently deprecated.
        inMemory : bool, optional
            Create dataset in memory. Filename is ignored then,
            but still needs to be specified.
        mode : str
            'r'  - Readonly, file must exist

            'r+' - Read/write, file must exist

            'w'  - Create file, overwrite if exists

            'w-' - Create file, fail if exists

            'a'  - Read/write if exists, create otherwise (default)
        """
        #-->>> Important::: do not define any variables before vectors!!! <<<--
        #These are fields that will be kept on a hard drive
        #You can learn what variables mean from here too.
        self.vectors = {
            # chromosomes for each read.
            "chrms1": "int8", "chrms2": "int8",

            "mids1": "int32", "mids2": "int32",
            # midpoint of a fragment, determined as "(start+end)/2"

            "fraglens1": "int32", "fraglens2": "int32",
            # fragment lengthes

            "distances": "int32",
            # distance between fragments. If -1, different chromosomes.
            #If -2, different arms.

            "fragids1": "int64", "fragids2": "int64",
            # IDs of fragments. fragIDmult * chromosome + location
            # distance to rsite
            "dists1": "int32", "dists2": "int32",
            # precise location of cut-site
            "cuts1": "int32", "cuts2": "int32",
            "strands1": "bool", "strands2": "bool",
            "DS": "bool", "SS": "bool",
            }
            #'rfragIdxs1': 'int32',
            #'rfragIdxs2':'int32',
            #'absRfragIdxs1':'int32',
            #'absRfragIdxs2':'int32'}

        #--------Deprecation warnings-------
        if override != "deprecated":
            warnings.warn(UserWarning("Please use a more intuitive flag "
                                      "'mode =' instead of 'override ='"))
            mode = 'w'
        elif override == True:
            mode = "w"
        if autoFlush != "deprecated":
            warnings.warn(UserWarning("Autoflush was deprecated, "
                                      "inMemory is adviced instead "
                                      "when memory is not an issue"))

        #-------Initialization of the genome and parameters-----
        if type(genome) == str:
            self.genome = Genome(genomePath=genome, readChrms=["#", "X"])
        else:
            self.genome = genome
        assert isinstance(self.genome, Genome)  # bla

        self.chromosomeCount = self.genome.chrmCount
        self.fragIDmult = self.genome.fragIDmult  # used for building heatmaps
        print "----> New dataset opened, genome %s, filename = %s" % (
            self.genome.folderName, filename)

        self.maximumMoleculeLength = maximumMoleculeLength
        # maximum length of a molecule for SS reads

        self.filename = os.path.abspath(os.path.expanduser(filename))  # File to save the data
        self.chunksize = 5000000
        # Chunk size for h5dict operation, external sorting, etc.

        self.inMemory = inMemory

        #------Creating filenames, etc---------
        if os.path.exists(self.filename) and (mode in ['w', 'a']):
            print '----->!!!File already exists! It will be {0}\n'.format(
                {"w": "deleted", "a": "opened in the append mode"}[mode])

        if len(os.path.split(self.filename)[0]) != 0:
            if not os.path.exists(os.path.split(self.filename)[0]):
                warnings.warn("Folder in which you want to create file"
                        "do not exist: %s" % os.path.split(self.filename)[0])
                try:
                    os.mkdir(os.path.split(self.filename)[0])
                except:
                    raise IOError("Failed to create directory: %s" %
                                  os.path.split(self.filename)[0])

        self.h5dict = h5dict(self.filename, mode=mode, in_memory=inMemory)
        if "DS" in self.h5dict.keys():
            DS = self.DS
            self.N = len(DS)
            self.DSnum = DS.sum()

    def _setData(self, name, data):
        "an internal method to save numpy arrays to HDD quickly"
        if name not in self.vectors.keys():
            raise ValueError("Attept to save data not "
                             "specified in self.vectors")
        dtype = np.dtype(self.vectors[name])
        data = np.asarray(data, dtype=dtype)
        self.h5dict[name] = data

    def _getData(self, name):
        "an internal method to load numpy arrays from HDD quickly"
        if name not in self.vectors.keys():
            raise ValueError("Attept to load data not "
                             "specified in self.vectors")
        return self.h5dict[name]

    def __getattribute__(self, x):
        """a method that overrides set/get operation for self.vectors
        o that they're always on HDD"""
        if x == "vectors":
            return object.__getattribute__(self, x)

        if x in self.vectors.keys():
            a = self._getData(x)
            return a
        else:
            return object.__getattribute__(self, x)

    def __setattr__(self, x, value):
        """a method that overrides set/get operation for self.vectors
        so that they're always on HDD"""
        if x == "vectors":
            return object.__setattr__(self, x, value)

        if x in self.vectors.keys():
            self._setData(x, value)
        else:
            return object.__setattr__(self, x, value)

    def _buildFragments(self):
        if not hasattr(self, "ufragments"):
            self.rebuildFragments()

    def _moveSSReads(self):
        mask = (self.chrms1 < 0)  # moving SS reads to the first side
        mask  # Eclipse warning removal
        variables = set([i[:-1] for i in self.vectors.keys(
        ) if i[-1] == "1"])  # set of variables to change
        for i in variables:
            exec("a = self.%s1" % i)
            exec("b = self.%s2" % i)
            exec("a[mask] = b[mask]")
            exec("b[mask] = -1")
            exec("self.%s1 = a" % i)
            exec("self.%s2 = b" % i)

    def evaluate(self, expression, internalVariables, externalVariables={},
                 constants={"np": np},
                 outVariable="autodetect",
                 chunkSize="default"):
        """
        Still experimental class to perform evaluation of
        any expression on hdf5 datasets
        Note that out_variable should be writable by slices.

        ---If one can provide autodetect of values for internal
        variables by parsing an expression, it would be great!---

        .. note ::
            See example of usage of this class in filterRsiteStart,
            parseInputData, etc.

        .. warning ::
            Please avoid passing internal variables
            as "self.cuts1" - use "cuts1"

        .. warning ::
            You have to pass all the modules and functions (e.g. np)
            in a "constants" dictionary.

        Parameters
        ----------
        expression : str
            Mathematical expression, single or multi line
        internal_variables : list of str
            List of variables ("chrms1", etc.), used in the expression
        external_variables : dict , optional
            Dict of {str:array}, where str indicates name of the variable,
            and array - value of the variable.
        constants : dict, optional
            Dictionary of constants to be used in the evaluation.
            Because evaluation happens without namespace,
            you should include numpy here if you use it (included by default)
        out_variable : str or tuple or None, optional
            Variable to output the data. Either internal variable, or tuple
            (name,value), where value is an array
        """
        if type(internalVariables) == str:
            internalVariables = [internalVariables]
        if chunkSize == "default":
            chunkSize = self.chunksize

        # detecting output variable automatically
        if outVariable == "autodetect":
            outVariable = expression.split("\n")[-1].split("=")[0].strip()
            if outVariable not in self.vectors:
                outVariable = (outVariable, "ToDefine")

        code = compile(expression, '<string>', 'exec')
            #compile because we're launching it many times

        bins = range(0, self.N, chunkSize) + [self.N]
            #creating bins to perform evaluation
        bins = zip(bins[:-1], bins[1:])
        for start, end in bins:

            variables = copy(constants)
            # dictionary to pass to the evaluator.
            #It's safer than to use the default locals()

            for name in internalVariables:
                if name not in self.h5dict.keys():
                    raise ValueError("{0} not in keys".format(name))
                variables[name] = self.h5dict.get_dataset(name)[start:end]

            for name, variable in externalVariables.items():
                variables[name] = variable[start:end]

            # actually execute the code in our own namespace
            exec code in variables

            # autodetecting output dtype on the first run if not specified
            if outVariable[1] == "ToDefine":
                dtype = variables[outVariable[0]].dtype
                outVariable = (outVariable[0], np.zeros(self.N, dtype))

            if type(outVariable) == str:
                self.h5dict.get_dataset(outVariable)[start:end] = variables[outVariable]
            elif len(outVariable) == 2:
                outVariable[1][start:end] = variables[outVariable[0]]
            elif outVariable is None:
                pass
            else:
                raise ValueError("Please provide str or (str,value)"
                                 " for out variable")

        if type(outVariable) == tuple:
            return outVariable[1]

    def flush(self):
        warnings.warn(UserWarning(
            "Autoflush was deprecated, flush is useless now"))

    def merge(self, filenames):
        """combines data from multiple datasets

        Parameters
        ----------
            filenames : list of strings
                List of folders to merge to current working folder
        """
        if self.filename in filenames:
            raise StandardError("----> Cannot merge folder into itself! "
                                "Create a new folder")
        for filename in filenames:
            if not os.path.exists(filename):
                raise IOError("\nCannot open file: %s" % filename)

        h5dicts = [h5dict(i, mode='r') for i in filenames]
        for name in self.vectors.keys():
            res = []
            for mydict in h5dicts:
                res.append(mydict[name])
            res = np.concatenate(res)
            self.N = len(res)
            if name == "DS":
                self.DSnum = res.sum()
            self._setData(name, res)

        self.rebuildFragments()

    def parseInputData(self, dictLike, zeroBaseChrom=True,
                       enzymeToFillRsites=None, removeSS=False, **kwargs):
        """Inputs data from a dictionary-like object,
        containing coordinates of the reads.
        Performs filtering of the reads.

        A good example of a dict-like object is a numpy.savez

        .. warning::
            Restriction fragments MUST be specified
            exactly as in the Genome class.

        .. warning::
            Strand information is needed for proper scaling
            calculations, but will be imitated if not provided

        Parameters
        ----------
        dictLike : dict or dictLike object, or string with h5dict filename
            Input reads

        dictLike["chrms1,2"] : array-like
            Chromosomes of 2 sides of the read
        dictLike["cuts1,2"] :  array-like
            Exact position of cuts
        dictLike["strands1,2"], essential : array-like
            Direction of the read
        dictLike["rsites1,2"], optional : array-like
            Position of rsite to which the read is pointing
        dictLike["uprsites1,2"] , optional : array-like
            rsite upstream (larger genomic coordinate) of the cut position
        dictLike["downrsites1,2"] , optional : array-like
            rsite downstream (smaller genomic coordinate) of the cut position

        zeroBaseChrom : bool , optional
            Use zero-base chromosome counting if True, one-base if False
        enzymeToFillRsites : None or str, optional if rsites are specified
            Enzyme name to use with Bio.restriction
        removeSS : bool, optional
            If set to True, removes SS reads from the library
        noFiltering : bool, optional
            If True then no filters are applied to the data. False by default.
            Overrides removeSS. Experimental, do not use if you are not sure.
        """

        rsite_related = ["rsites1", "rsites2", "uprsites1",
                         "uprsites2", "downrsites1", "downrsites2"]
        if type(dictLike) == str:
            if not os.path.exists(dictLike):
                raise IOError("File not found: %s" % dictLike)
            print "     loading data from file %s (assuming h5dict)" % dictLike
            dictLike = h5dict(dictLike, 'r')  # attempting to open h5dict

        if all([i in dictLike.keys() for i in rsite_related]):
            noRsites = False
        else:
            noRsites = True

        "Filling in chromosomes and positions - mandatory objects"
        a = dictLike["chrms1"]
        self.trackLen = len(a)

        if zeroBaseChrom == True:
            self.chrms1 = a
            self.chrms2 = dictLike["chrms2"]
        else:
            self.chrms1 = a - 1
            self.chrms2 = dictLike["chrms2"] - 1
        self.N = len(self.chrms1)
        del a

        self.cuts1 = dictLike['cuts1']
        self.cuts2 = dictLike['cuts2']
        
        if not (("strands1" in dictLike.keys()) and
                ("strands2" in dictLike.keys())):
            warnings.warn("No strand information provided,"
                          " assigning random strands.")
            t = np.random.randint(0, 2, self.trackLen)
            self.strands1 = t
            self.strands2 = 1 - t
            del t
            noStrand = True
        else:
            self.strands1 = dictLike["strands1"]
            self.strands2 = dictLike["strands2"]
            noStrand = False  # strand information filled in

        # We have to fill rsites ousrlves. Let's see what enzyme to use!
        if noRsites == True:
            if ((enzymeToFillRsites is None) and
                (self.genome.hasEnzyme() == False)):
                raise ValueError("Please specify enzyme"
                                 " if your data has no rsites")

            if enzymeToFillRsites is not None:
                if self.genome.hasEnzyme() == True:
                    if enzymeToFillRsites != self.genome.enzymeName:
                        warnings.warn("genome.enzymeName "
                                      "different from supplied enzyme")
                self.genome.setEnzyme(enzymeToFillRsites)
            #enzymeToFillRsites has preference over self.genome's enzyme

            print "Filling rsites"
            rsitedict = h5dict(in_memory=self.inMemory)
                #creating dict to pass to fillRsite's code
            rsitedict["chrms1"] = self.chrms1
            rsitedict["chrms2"] = self.chrms2
            rsitedict["cuts1"] = self.cuts1
            rsitedict["cuts2"] = self.cuts2
            rsitedict["strands1"] = self.strands1
            rsitedict["strands2"] = self.strands2
            hiclib.mapping.fill_rsites(
                lib=rsitedict, genome_db=self.genome)
        else:
            rsitedict = dictLike  # rsite information is in our dictionary

        self.DS = (self.chrms1 >= 0) * (self.chrms2 >= 0)
            #if we have reads from both chromosomes, we're a DS read
        self.SS = (self.DS == False)

        self.dists1 = np.abs(rsitedict["rsites1"] - self.cuts1)
        self.dists2 = np.abs(rsitedict["rsites2"] - self.cuts2)

        self.mids1 = (rsitedict["uprsites1"] + rsitedict["downrsites1"]) / 2
        self.mids2 = (rsitedict["uprsites2"] + rsitedict["downrsites2"]) / 2

        self.fraglens1 = np.abs(
            (rsitedict["uprsites1"] - rsitedict["downrsites1"]))
        self.fraglens2 = np.abs(
            (rsitedict["uprsites2"] - rsitedict["downrsites2"]))

        del rsitedict  # deletes hdf5 file, so it's important

        self.fragids1 = self.mids1 + np.array(self.chrms1,
                                              dtype="int64") * self.fragIDmult
        self.fragids2 = self.mids2 + np.array(self.chrms2,
                                              dtype="int64") * self.fragIDmult

        distances = np.abs(self.mids1 - self.mids2)
        distances[self.chrms1 != self.chrms2] = -1
        self.distances = distances  # distances between restriction fragments
        del distances

        self._moveSSReads()  # Eclipse warning removal
        try:
            dictLike["misc"]["genome"]["idx2label"]
            self.updateGenome(self.genome, removeSSreads="trans",
                              oldGenome=dictLike["misc"]["genome"]["idx2label"])
        except KeyError:
            assumedGenome = Genome(self.genome.genomePath)
            self.updateGenome(self.genome, removeSSreads="trans",
                              oldGenome=assumedGenome)
            warnings.warn("\n Genome not found in mapped data. \n"
            "Assuming genome comes from the same folder with all chromosomes")

        # Discard dangling ends and self-circles
        mask = self.evaluate("a = (fragids1 != fragids2)",
                             ["fragids1", "fragids2"])
        maskLen, noSameFrag = len(mask), mask.sum()

        # Discard unused chromosomes
        mask *= ((self.chrms1 < self.chromosomeCount) *
                 (self.chrms2 < self.chromosomeCount))
        noUnusedChroms = mask.sum()
        if removeSS == False:
            mask *= ((self.chrms1 >= 0) + (self.chrms2 >= 0))
                #Has to have at least one side mapped
        else:
            mask *= ((self.chrms1 >= 0) * (self.chrms2 >= 0))
                #Has to have both sides mapped
        noUnmapped = mask.sum()
        if noStrand == True:
            # Can't tell if reads point to each other.
            dist = self.evaluate("a = np.abs(cuts1 - cuts2)",
                                 ["cuts1", "cuts2"])
        else:
            # distance between sites facing each other
            dist = self.evaluate("a = - cuts1 * (2 * strands1 -1) - "
                                 "cuts2 * (2 * strands2 - 1)",
                                 ["cuts1", "cuts2", "strands1", "strands2"])

        readsMolecules = self.evaluate(
           "a = (chrms1 == chrms2)*(strands1 != strands2) *  (dist >=0) *"
           " (dist <= maximumMoleculeLength)",
           internalVariables=["chrms1", "chrms2", "strands1", "strands2"],
           externalVariables={"dist": dist},
           constants={"maximumMoleculeLength": self.maximumMoleculeLength})
        mask *= (readsMolecules == False)
        extraDE = mask.sum()

        print "     Original reads: {maskLen}  -> "\
        "No same fragment: {noSameFrag} -> "\
        "remove unused chrom: {noUnusedChroms} -> ...".format(**locals())

        print "     ... -> No unmapped reads: {noUnmapped} -> "\
        "no extra DEs (--> (<500) <--): {extraDE}".format(**locals())

        if mask.sum() == 0:
            raise Exception(
                'No reads left after filtering. Please, check the input data')

        del dist
        del readsMolecules
        if not kwargs.get('noFiltering', False):
            self.maskFilter(mask)

    def saveFragments(self):
        """saves fragment data to make correct expected
        estimates after applying a heavy mask"""
        self.ufragmentsOriginal = np.array(self.ufragments)
        self.ufragmentlenOriginal = np.array(self.ufragmentlen)

    def originalFragments(self):
        "loads original fragments"
        self.ufragments = np.array(self.ufragmentsOriginal)
        self.ufragmentlen = np.array(self.ufragmentlenOriginal)

    def updateGenome(self, newGenome, removeSSreads="trans",
                     oldGenome="current"):
        """
        Updates dataset to a new genome, with a fewer number of chromosomes.
        Use it to delete chromosomes.
        By default, removes all DS reads with that chromosomes.

        Parameters
        ----------
        newGenome : Genome object
            Genome to replace the old genome, with fewer chromosomes
        removeSSreads : "trans"(default), "all" or "none"
            "trans": remove all reads from deleted chromosomes,
            ignore the rest.
            "all": remove all SS reads from all chromosomes
            "None": mark all trans reads as SS reads

        oldGenome : Genome object or idx2label of old genome, optional

        """

        assert isinstance(newGenome, Genome)
        newN = newGenome.chrmCount
        if oldGenome == "current":
            oldGenome = self.genome
        upgrade = newGenome.upgradeMatrix(self.genome)
        try:
            oldN = oldGenome.chrmCount
        except AttributeError:
            oldN = len(oldGenome.keys())
        if oldN == newN:
            return None

        if upgrade is not None:
            upgrade[upgrade == -
                    1] = 9999  # to tell old SS reads from new SS reads
            chrms1 = np.array(self.chrms1, int)
            mask = chrms1 >= 0
            chrms1[mask] = upgrade[chrms1[mask]]
            self.chrms1 = chrms1
            del chrms1
            chrms2 = np.array(self.chrms2, int)
            mask = chrms2 >= 0
            chrms2[mask] = upgrade[chrms2[mask]]
            self.chrms2 = chrms2

        if isinstance(oldGenome, Genome):
            if oldGenome.hasEnzyme():
                newGenome.setEnzyme(oldGenome.enzymeName)

        if removeSSreads.lower() == "all":
            "Keeping only DS reads"
            mask = ((self.chrms1 < newN) * (self.chrms2 < newN) * self.DS)
            self.genome = newGenome
            self.maskFilter(mask)

        if removeSSreads.lower() == "none":
            "Maksin all trans and SS reads into new SS reads"
            self.maskFilter((self.chrms1 >= 0) * (self.chrms1 < newN) +
                            (self.chrms2 >= 0) * (self.chrms2 < newN))
            self.chrms1[self.chrms1 >= newN] = -1
            self.chrms1[self.chrms2 >= newN] = -1
            self._moveSSReads()

            # if we have reads from both chromosomes, we're a DS read
            self.DS = (self.chrms1 >= 0) * (self.chrms2 >= 0)
            self.SS = (self.DS == False)

        if removeSSreads.lower() == "trans":
            "Removing trans reads, keeping SS as they are"
            self.maskFilter((self.chrms1 < newN) * (self.chrms2 < newN) *
                            self.DS + self.SS * (self.chrms1 >= 0) *
                            (self.chrms1 < newN))

    def calculateWeights(self):
        """Calculates weights for reads based on fragment length correction
         similar to Tanay's;
         may be used for scalings or creating heatmaps"""
        self._buildFragments()
        fragmentLength = self.ufragmentlen
        pls = np.sort(fragmentLength)
        pls = np.r_[pls, pls[-1] + 1]
        N = len(fragmentLength)
        mysum = np.array(self.fragmentSum(), float)
        self.weights = np.ones(N, float)
        meanSum = np.mean(np.array(mysum, float))
        #watch = np.zeros(len(mysum),int)
        for i in np.arange(0, 0.991, 0.01):
            b1, b2 = pls[i * N], pls[(i + 0.01) * N]
            p = (b1 <= fragmentLength) * (b2 > fragmentLength)
            #watch[p] += 1
            value = np.mean(mysum[p])
            if p.sum() > 0:
                self.weights[p] = value / meanSum
            else:
                print "no weights", i, b1, b2

    def buildHeatmap(self, chromosome=14, chromosome2=None, resolution=1000000,
                     show=False, useRsiteDensity=True):
        """Builds heatmaps between any two chromosomes at a given resolution

        .. warning::
            This method will be deprecated on 11/01/2012

        Parameters
        ----------
        chromosome1 : int
            First chromosome
        chromosome2 : int, optional
            Second chromosome, if the map is trans.
        resolution : int
            Resolution of a heatmap. Default is 1M
        show : bool , optional
            Show heatmap, or just output it?
        useRsiteDensity : bool, optional
            Correct map by density of rsites.

        """
        #TODO:(MI) Deprecate on 11/01/2012

        w = UserWarning("This method will be deprecated on 11/01/2012"
                        "Use saveByChromosomeHeatmap instead"
                        "Or write me an email")
        warnings.warn(w)

        self._buildFragments()
        if chromosome2 is None:
            chromosome2 = chromosome
            mask = (self.chrms1 == chromosome) * (self.chrms2 == chromosome)
            p1 = self.mids1[mask]
            p2 = self.mids2[mask]
            p1 = na(p1, int)
            p2 = na(p2, int)
            b1 = np.arange(0, max(p1.max() + resolution,
                                  p2.max() + resolution), resolution)
            hist = np.histogram2d(p1, p2, (b1))[0]
            hist = hist + np.transpose(hist)

        else:
            mask = (self.chrms1 == chromosome) * (self.chrms2 == chromosome2)
            p11 = self.mids1[mask]
            p21 = self.mids2[mask]
            mask = (self.chrms1 == chromosome2) * (self.chrms2 == chromosome)
            p12 = self.mids2[mask]
            p22 = self.mids1[mask]
            p1 = np.r_[p11, p12]
            p2 = np.r_[p21, p22]
            if (len(p1) == 0) or (len(p2) == 0):
                return np.zeros((500, 500), float) + 0.000001
            b1 = np.arange(0, (p1.max() + resolution), resolution)
            b2 = np.arange(0, (p2.max() + resolution), resolution)
            l1 = len(b1)
            l2 = len(b2)
            hist = np.histogram2d(p1, p2, (b1, b2))[0]

        if chromosome2 is None:
            chromosome2 = chromosome
        if useRsiteDensity == True:
            m1 = self.ufragments / self.fragIDmult == chromosome
            m2 = self.ufragments / self.fragIDmult == chromosome2
            p1 = self.ufragments[m1] % self.fragIDmult
            p2 = self.ufragments[m2] % self.fragIDmult
            p1mod = p1 / resolution
            p2mod = p2 / resolution
            myarray = np.array(range(np.max(p1mod) + 1))
            vec1 = sumByArray(p1mod, myarray)
            myarray = np.array(range(np.max(p2mod) + 1))
            vec2 = sumByArray(p2mod, myarray)
            vec1 = vec1[:l1]
            vec2 = vec2[:l2]
            correction = na(vec1[:, None] * vec2[None, :], float)
            mask = np.logical_or(np.isnan(correction), correction == 0)
            correction[mask] = 1
            hist2 = hist / correction
            hist2 /= (np.mean(hist2[mask == False]) / np.mean(
                hist[mask == False]))
            if show == True:
                mat_img(np.log(np.array(hist2, float)))
            else:
                return np.array(hist2)
        if show == True:
            mat_img(np.log(np.array(hist, float)))
        else:
            return np.array(hist)

    def buildAllHeatmap(self, resolution, countDiagonalReads="Once"):
        """Creates an all-by-all heatmap in accordance with mapping
        provided by 'genome' class

        Parameters
        ----------
        resolution : int
            Resolution of a heatmap
        countDiagonalReads : "once" or "twice"
            How many times to count reads in the diagonal bin
        """
        #8 bytes per record + heatmap
        self.genome.setResolution(resolution)
        dr = self.DS
        label = self.genome.chrmStartsBinCont[self.chrms1[dr]]
        label = np.asarray(label, dtype="int64")
        label += self.mids1[dr] / resolution
        label *= self.genome.numBins
        label += self.genome.chrmStartsBinCont[self.chrms2[dr]]
        label += self.mids2[dr] / resolution

        counts = np.bincount(label, minlength=self.genome.numBins ** 2)
        if len(counts) > self.genome.numBins ** 2:
            raise StandardError("\nheatmap exceed length of the genome!!!"
                                " Check genome")

        counts.shape = (self.genome.numBins, self.genome.numBins)
        for i in xrange(len(counts)):
            counts[i, i:] += counts[i:, i]
            counts[i:, i] = counts[i, i:]
        if countDiagonalReads.lower() == "once":
            diag = np.diag(counts)
            fillDiagonal(counts, diag / 2)
        elif countDiagonalReads.lower() == "twice":
            pass
        else:
            raise ValueError("Bad value for countDiagonalReads")
        return counts

    def buildSinglesCoverage(self, resolution):
        """creates an SS coverage vector heatmap in accordance with the
        output of the 'genome' class"""
        self.genome.setResolution(resolution)
        ds = self.DS == False
        if ds.sum() == 0:
            return np.zeros(self.genome.numBins)
        label = self.genome.chrmStartsBinCont[self.chrms1[ds]] + \
            self.mids1[ds] / resolution
        label = np.asarray(label, dtype="int64")
        counts = np.bincount(label, minlength=self.genome.numBins)
        return counts

    def buildFragmetCoverage(self, resolution):
        """creates restriction site density vector (visible sites only)
        in accordance with the 'genome' class"""
        self._buildFragments()
        self.genome.setResolution(resolution)
        chroms = self.ufragments / self.fragIDmult
        positions = self.ufragments % self.fragIDmult
        label = self.genome.chrmStartsBinCont[chroms] + positions / resolution
        counts = np.bincount(label, minlength=self.genome.numBins)
        return counts

    def fragmentFilter(self, fragments):
        """keeps only reads that originate from fragments in 'fragments'
        variable, for DS - on both sides

        Parameters
        ----------
        fragments : numpy.array of fragment IDs or bools
            List of fragments to keep, or their indexes in self.ufragments
        """
        if fragments.dtype == np.bool:
            self._buildFragments()
            fragments = self.ufragments[fragments]
        m1 = arrayInArray(self.fragids1, fragments)
        m2 = arrayInArray(self.fragids2, fragments) + self.SS
        mask = np.logical_and(m1, m2)
        self.maskFilter(mask)

    def maskFilter(self, mask):
        """keeps only reads designated by mask

        Parameters
        ----------
        mask : array of bools
            Indexes of reads to keep
        """
        #Uses 16 bytes per read

        print "          Number of reads changed  %d ---> %d" % (
            len(mask), mask.sum()),
        length = 0
        ms = mask.sum()
        assert mask.dtype == np.bool
        self.N = ms
        if hasattr(self, "ufragments"):
            del self.ufragmentlen, self.ufragments
        for name in self.vectors:
            data = self._getData(name)
            ld = len(data)
            if length == 0:
                length = ld
            else:
                if ld != length:
                    self.delete()
            newdata = fasterBooleanIndexing(data, mask, outLen=ms,
                                        bounds=False)  # see mirnylib.numutils
            #newdata = data[mask]
            del data
            self._setData(name, newdata)
            if name == "DS":
                self.DSnum = int(newdata.sum())
            del newdata
        del mask
        self.rebuildFragments()

    def rebuildFragments(self):
        "recreates a set of fragments - runs when reads have changed"
        try:
            past = len(self.ufragments)
        except:
            past = 0
        assert len(self.fragids2) == len(self.DS)
        ufragids1, ufragids1ind = chunkedUnique(self.fragids1,
            return_index=True, chunksize=self.chunksize)
        ufragids2, ufragids2ind = chunkedUnique(
            fasterBooleanIndexing(self.fragids2, self.DS, outLen=self.DSnum),
            return_index=True, chunksize=self.chunksize)

        #Funding unique fragments and unique fragment IDs
        ufragment1len = self.fraglens1[ufragids1ind]
        ufragment2len = self.fraglens2[self.DS][ufragids2ind]

        uall = np.r_[ufragids1, ufragids2]
        ulen = np.r_[ufragment1len, ufragment2len]

        self.ufragments, ind = np.unique(uall, True)
        self.ufragmentlen = ulen[ind]
        print "          Fragments number changed -   %d --->  %d" % (
            past, len(self.ufragments))

    def filterExtreme(self, cutH=0.005, cutL=0):
        """removes fragments with most and/or least # counts

        Parameters
        ----------
        cutH : float, 0<=cutH < 1, optional
            Fraction of the most-counts fragments to be removed
        cutL : float, 0<=cutL<1, optional
            Fraction of the least-counts fragments to be removed
        """
        self._buildFragments()
        print "----->Extreme fragments filter: remove top %lf, "\
        "bottom %lf fragments" % (cutH, cutL)

        s = self.fragmentSum()
        ss = np.sort(s)

        valueL, valueH = np.percentile(ss, [100. * cutL, 100 * (1. - cutH)])
        news = (s >= valueL) * (s <= valueH)
        self.fragmentFilter(self.ufragments[news])

        print "     #Top fragments are: ", ss[-10:]
        print "     # Cutoff for low # counts is (counts): ", valueL,
        print  "; cutoff for large # counts is: ", valueH, "\n"

    def filterLarge(self, cutlarge=100000, cutsmall=100):
        """removes very large and small fragments

        Parameters
        ----------
        cutlarge : int
            remove fragments larger than it
        cutsmall : int
            remove fragments smaller than it
        """
        self._buildFragments()
        print "----->Small/large fragments filter: keep strictly less"\
        "than %d,strictly more than %d bp" % (cutlarge, cutsmall)
        p = (self.ufragmentlen < (cutlarge)) * (self.ufragmentlen > cutsmall)
        self.fragmentFilter(self.ufragments[p])
        print

    def filterRsiteStart(self, offset=5):
        """Removes reads that start within x bp near rsite

        Parameters
        ----------

        offset : int
            Number of bp to exclude next to rsite, not including offset

        """

        #TODO:(MI) fix this so that it agrees with the definition.

        print "----->Semi-dangling end filter: remove guys who start %d"\
        " bp near the rsite" % offset

        expression = "mask = (np.abs(dists1 - fraglens1) >= offset) * "\
        "((np.abs(dists2 - fraglens2) >= offset) * DS + SS)"
        mask = self.evaluate(expression,
                             internalVariables=["dists1", "fraglens1",
                                                "dists2", "fraglens2",
                                                "SS", "DS"],
                             constants={"offset": offset, "np": np},
                             outVariable=("mask", np.zeros(self.N, bool)))

        #Old code, if something fails, switch to it!!!
        #d1 = np.abs(self.dists1 - self.fraglens1) >= offset
        #d2 = np.abs(self.dists2 - self.fraglens2) >= offset
        #mask =  d1 * (d2 * self.DS + self.SS)
        #mask = (np.abs(self.dists1 - self.fraglens1) >=offset) * \
        #((np.abs(self.dists2 - self.fraglens2) >= offset )* self.DS + self.SS)
        self.maskFilter(mask)
        print

    def filterDuplicates(self):
        "removes duplicate molecules in DS reads"
        "TODO: rewrite it when Anton allows direct creation of Hi-C datasets"
        #Uses a lot!
        print "----->Filtering duplicates in DS reads: "
        DS = self.DS
        Nds = DS.sum()

        # an array to determine unique rows. Eats 16 bytes per DS record
        dups = np.zeros((Nds, 2), dtype="int64", order="C")

        dups[:, 0] = self.chrms1[DS]
        dups[:, 0] *= self.fragIDmult
        dups[:, 0] += self.cuts1[DS]
        dups[:, 1] = self.chrms2[DS]
        dups[:, 1] *= self.fragIDmult
        dups[:, 1] += self.cuts2[DS]
        dups.shape = (Nds * 2)
        strings = dups.view("|S16")
            #Converting two indices to a single string to run unique
        uids = uniqueIndex(strings)
        del strings, dups
        stay = self.SS  # mask
        putIn = np.zeros(Nds, bool)  # mask inside DS mask
        putIn[uids] = True  # indexes of unique DS elements
        stay[DS] = putIn  # filling in mask of all reads
        print "     Number of DS reads changed - %d ---> %d" % (
            Nds, Nds - len(DS) + stay.sum())
        del uids
        uflen = len(self.ufragments)
        self.maskFilter(stay)
        assert len(self.ufragments) == uflen
        print

    def fragmentSum(self, fragments=None, strands="both"):
        """returns sum of all counts for a set or subset of fragments


        Parameters
        ----------
        fragments : list of fragment IDs, optional
            Use only this fragments. By default all fragments are used
        strands : 1,2 or "both" (default)
            Use only first or second side of the read
            (first has SS, second - doesn't)
        """
        #Uses 16 bytes per read
        self._buildFragments()
        if fragments is None:
            fragments = self.ufragments

        if strands == "both":
            return sumByArray(self.fragids1, fragments) + \
                sumByArray(fasterBooleanIndexing(self.fragids2, self.DS,
                                                 outLen=self.DSnum),
                           fragments)
        if strands == 1:
            return sumByArray(self.fragids1, fragments)
        if strands == 2:
            return sumByArray(fasterBooleanIndexing(self.fragids2, self.DS),
                              fragments)

    def printStats(self):
        #Uses <10 bytes per read
        print "-----> Statistics for the file  %s!" % self.filename
        print "     Single sided reads: ", self.SS.sum()
        print "     Double sided reads: ", self.DS.sum()
        ss1 = self.strands1[self.chrms1 >= 0]
        ss2 = self.strands2[self.chrms2 >= 0]
        sf = ss1.sum() + ss2.sum()
        sr = len(ss1) + len(ss2) - sf
        print "     reverse/forward bias", float(sr) / sf

    def save(self, filename):
        "Saves dataset to filename, does not change the working file."
        if self.filename == filename:
            raise StandardError("Cannot save to the working file")
        newh5dict = h5dict(filename, mode='w')
        for name in self.vectors.keys():
            newh5dict[name] = self.h5dict[name]
        print "----> Data saved to file %s" % (filename,)

    def load(self, filename, buildFragments=True):
        "Loads dataset from file to working file; check for inconsistency"
        otherh5dict = h5dict(filename, 'r')
        length = 0
        for name in self.vectors:
            data = otherh5dict[name]
            if name == "DS":
                self.DSnum = data.sum()
            ld = len(data)
            if length == 0:
                length = ld
            else:
                if ld != length:
                    print("---->!!!!!File %s contains inconsistend data<----" %
                          filename)
                    self.exitProgram("----> Sorry...")

            self._setData(name, data)
        print "---->Loaded data from file %s, contains %d reads" % (
            filename, length)
        self.N = length

        if buildFragments == True:
            self.rebuildFragments()

    def saveHeatmap(self, filename, resolution=1000000,
                    countDiagonalReads="Once"):
        """
        Saves heatmap to filename at given resolution.

        Parameters
        ----------
        filename : str
            Filename of the output h5dict
        resolution : int
            Resolution of an all-by-all heatmap
        countDiagonalReads : "once" or "twice"
            How many times to count reads in the diagonal bin

        """

        try:
            os.remove(filename)
        except:
            pass

        tosave = h5dict(path=filename, mode="w")
        heatmap = self.buildAllHeatmap(resolution, countDiagonalReads)
        tosave["heatmap"] = heatmap
        del heatmap
        singles = self.buildSinglesCoverage(resolution)
        frags = self.buildFragmetCoverage(resolution)
        chromosomeStarts = np.array(self.genome.chrmStartsBinCont)
        tosave["resolution"] = resolution
        tosave["singles"] = singles
        tosave["frags"] = frags
        tosave["genomeBinNum"] = self.genome.numBins
        tosave["genomeIdxToLabel"] = self.genome.idx2label
        tosave["chromosomeStarts"] = chromosomeStarts
        print "----> Heatmap saved to '%s' at %d resolution" % (
            filename, resolution)

    def saveByChromosomeHeatmap(self, filename, resolution=10000,
                                includeTrans=True,
                                countDiagonalReads="Once"):
        """
        Saves chromosome by chromosome heatmaps to h5dict.
        This method is not as memory demanding as saving allxall heatmap.

        Keys of the h5dict are of the format ["1 14"],
        where chromosomes are zero-based,
        and there is one space between numbers.

        .. warning :: Chromosome numbers are always zero-based.
        Only "chr3" labels are one-based in this package.

        Parameters
        ----------

        filename : str
            Filename of the h5dict with the output
        resolution : int
            Resolution to save heatmaps
        includeTrans : bool, optional
            Build inter-chromosomal heatmaps (default: False)
        countDiagonalReads : "once" or "twice"
            How many times to count reads in the diagonal bin

        """
        if countDiagonalReads.lower() not in ["once", "twice"]:
            raise ValueError("Bad value for countDiagonalReads")
        self.genome.setResolution(resolution)
        pos1 = self.evaluate("a = np.array(mids1 / {res}, dtype = 'int32')"
                             .format(res=resolution), "mids1")
        pos2 = self.evaluate("a = np.array(mids2 / {res}, dtype = 'int32')"
                             .format(res=resolution), "mids2")
        chr1 = self.chrms1
        chr2 = self.chrms2
        DS = self.DS  # 13 bytes per read up to now, 16 total
        mydict = h5dict(filename)

        for chrom in xrange(self.genome.chrmCount):
            if includeTrans == True:
                mask = ((chr1 == chrom) + (chr2 == chrom)) * DS
            else:
                mask = ((chr1 == chrom) * (chr2 == chrom))
            # Located chromosomes and positions of chromosomes
            c1, c2, p1, p2 = chr1[mask], chr2[mask], pos1[mask], pos2[mask]
            if includeTrans == True:
                #moving different chromosomes to c2
                #c1 == chrom now
                mask = (c2 == chrom) * (c1 != chrom)
                c1[mask], c2[mask], p1[mask], p2[mask] = c2[mask].copy(), c1[
                    mask].copy(), p2[mask].copy(), p1[mask].copy()
                del c1  # ignore c1
                args = np.argsort(c2)
                c2 = c2[args]
                p1 = p1[args]
                p2 = p2[args]

            for chrom2 in xrange(chrom, self.genome.chrmCount):
                if (includeTrans == False) and (chrom2 != chrom):
                    continue
                start = np.searchsorted(c2, chrom2, "left")
                end = np.searchsorted(c2, chrom2, "right")
                cur1 = p1[start:end]
                cur2 = p2[start:end]
                label = np.asarray(cur1, "int64")
                label *= self.genome.chrmLensBin[chrom2]
                label += cur2
                maxLabel = self.genome.chrmLensBin[chrom] * \
                           self.genome.chrmLensBin[chrom2]
                counts = np.bincount(label, minlength=maxLabel)
                assert len(counts) == maxLabel
                mymap = counts.reshape((self.genome.chrmLensBin[chrom], -1))
                if chrom == chrom2:
                    mymap = mymap + mymap.T
                    if countDiagonalReads.lower() == "once":
                        fillDiagonal(mymap, np.diag(mymap).copy() / 2)
                mydict["%d %d" % (chrom, chrom2)] = mymap
        return

    def exitProgram(self, a):
        print a
        print "     ----> Bye! :) <----"
        exit()

    def setRfragIdxs(self, rEnzyme=None):
        if self.genome.hasEnzyme() == False:
            if rEnzyme is None:
                raise Exception("Please specify the restriction enzyme.")
            else:
                self.genome.setEnzyme(rEnzyme)

        rfragIdxs1 = np.ones(self.N, dtype='int32') * -1
        rfragIdxs2 = np.ones(self.N, dtype='int32') * -1
        for i in range(max(self.chrms1.max(), self.chrms2.max()) + 1):
            allMids = (self.genome.rsites[i] + np.r_[0, self.genome.rsites[i][:-1]]) / 2
            mask1 = (self.chrms1 == i)
            rfragIdxs1[mask1] = np.searchsorted(
                allMids, self.mids1[mask1], side='left').astype('int32')
            mask2 = (self.chrms2 == i)
            rfragIdxs2[mask2] = np.searchsorted(
                allMids, self.mids2[mask2], side='left').astype('int32')
            print 'Chromosome #{0}: {1} out of {2} rfragIdxs are restored correctly'.format(
                i,
                (allMids[rfragIdxs1[mask1]] == self.mids1[mask1]).sum()
                + (allMids[rfragIdxs2[mask2]] == self.mids2[mask2]).sum(),
                mask1.sum() + mask2.sum())

        rfragAbsIdxs1 = self.genome.chrmStartsRfragCont[self.chrms1] + rfragIdxs1
        rfragAbsIdxs2 = self.genome.chrmStartsRfragCont[self.chrms2] + rfragIdxs2
        rfragAbsIdxs1[self.chrms1 == -1] = -1
        rfragAbsIdxs2[self.chrms2 == -1] = -1

        self.vectors['rfragIdxs1'] = 'int32'
        self.vectors['rfragIdxs2'] = 'int32'
        self.vectors['rfragAbsIdxs1'] = 'int32'
        self.vectors['rfragAbsIdxs2'] = 'int32'

        self.rfragIdxs1 = rfragIdxs1
        self.rfragIdxs2 = rfragIdxs2
        self.rfragAbsIdxs1 = rfragAbsIdxs1
        self.rfragAbsIdxs2 = rfragAbsIdxs2

    def iterativeCorrection(self, numsteps = 10, normToLen=False):
        '''
        This function performs fragment-based iterative correction of Hi-C data.
        '''
        if 'rfragAbsIdxs1' not in self.vectors:
            raise Exception('Run setRfragIdxs() first!')

        rfragLensConc = np.concatenate(self.genome.rfragLens)
        weights = np.ones(self.N, dtype=np.float32)
        concRfragAbsIdxs = np.r_[self.rfragAbsIdxs1, self.rfragAbsIdxs2]
        concOrigArgs = np.r_[np.arange(0, self.N), np.arange(0, self.N)]
        concArgs = np.argsort(concRfragAbsIdxs)
        concRfragAbsIdxs = concRfragAbsIdxs[concArgs]
        concOrigArgs = concOrigArgs[concArgs]
        fragBorders = np.where(concRfragAbsIdxs[:-1] != concRfragAbsIdxs[1:])[0] + 1
        fragBorders = np.r_[0, fragBorders, 2*self.N]
        rfragLensLocal = rfragLensConc[concRfragAbsIdxs[fragBorders[:-1]]]
        for step in range(numsteps):
            for i in range(len(fragBorders) - 1):
                mask = concOrigArgs[fragBorders[i]:fragBorders[i+1]]
                totWeight = weights[mask].sum()
                if normToLen:
                    weights[mask] *= rfragLensLocal[i] / totWeight
                else:
                    weights[mask] /= totWeight

        self.vectors['weights'] = 'float32'
        self.weights = weights

class HiCStatistics(HiCdataset):
    """a semi-experimental sub-class of a 'HiCdataset' class
     used to do statistics on Hi-C reads
     Please be careful.
     """

    def multiplyForTesting(self, N=100):
        """used for heavy load testing only
        """
        for name in self.vectors:
            print "multipliing", name
            one = self._getData(name)
            blowup = np.hstack(tuple([one] * N))
            if name in ["mids1", "mids2"]:
                blowup += np.random.randint(0, 2 * N, len(blowup))

            self._setData(name, blowup)

    def buildLengthDependencePlot(self, strands="both", normalize=True,
                                  **kwargs):
        """plots dependence of counts on fragment length.
        May do based on one strands only
        "please run  plt.legend & plt.show() after calling this
        for all datasets you want to consider"""
        self._buildFragments()
        fragmentLength = self.ufragmentlen
        pls = np.sort(fragmentLength)
        N = len(fragmentLength)
        sums = []
        sizes = []
        mysum = self.fragmentSum(None, strands)

        for i in np.arange(0, 0.98, 0.015):
            b1, b2 = pls[i * N], pls[(i + 0.015) * N - 1]
            p = (b1 < fragmentLength) * (b2 > fragmentLength)
            value = np.mean(mysum[p])
            sums.append(value)
            sizes.append(np.mean(fragmentLength[p]))
        sums = np.array(sums)
        if normalize == True:
            sums /= sums.mean()
        plt.plot(sizes, sums, **kwargs)

    def plotScaling(self, fragids1=None, fragids2=None,
                    # IDs of fragments for which to plot scaling.
                    #One can, for example, limit oneself to
                    #only fragments shorter than 1000 bp
                    #Or calculate scaling only between different arms

                    useWeights=False,
                        # use weights associated with fragment length
                    excludeNeighbors=None, enzyme=None,
                        # number of neighboring fragments to exclude.
                        #Enzyme is needed for that!
                    normalize=True, normRange=None,
                        # normalize the final plot to sum to one
                    withinArms=True,
                        #Treat chromosomal arms separately
                    mindist=10000,
                        # Scaling was proved to be unreliable
                        # under 10000 bp for 6-cutter enzymes
                    maxdist=None,

                    #----Calculating scaling within a set of regions only----
                    regions=None,
                    # Array of tuples (chrom, start, end)
                    # for which scaling should be calculated
                    #Note that calculation might be extremely long
                    #(it might be proportional to # of regions for # > 100)

                    appendReadCount=True, **kwargs
                        # Append read count to the plot label
                        # kwargs to be passed to plotting
                    ):  # Sad smiley, because this method
                        # is very painful and complicated
        """plots scaling over, possibly uses subset of fragmetns, or weigts,
        possibly normalizes after plotting

        Plan of scaling calculation:

        1. Subdivide all genome into regions. \n
            a. Different chromosomes \n
            b. Different arms \n
            c. User defined squares/rectangles on a contact map \n
               -(chromosome, start,end) square around the diagonal \n
               -(chr, st1, end1, st2, end2) rectangle \n

        2. Use either all fragments, or only interactions between
        two groups of fragments \n
            e.g. you can calculate how scaling for small fragments is different
            from that for large \n
            It can be possibly used for testing Hi-C protocol issues. \n
            One can see effect of weights by doing this \n

        3. (optional) Calculate correction associated
        with fragment length dependence

        4. Subdivide all possible genomic separation into log-spaced bins

        5. Calculate expected number of fragment pairs within each bin
        (possibly with weights from step 3).

        If exclusion of neighbors is specificed,
        expected number of fragments knows about this

        Parameters
        ----------
        fragids1, fragids2 : np.array of fragment IDs, optional
            Scaling is calculated only for interactions between
            fragids1 and fragids2
            If omitted, all fragments are used
            If boolean array is supplied, it serves as a mask for fragments.
        useWeights : bool, optional
            Use weights calculated from fragment length
        excludeNeighbors : int or None, optional
            If None, all fragment pairs are considered.
            If integer, only fragment pairs separated
            by at least this number of r-fragments are considered.
        enzyme : string ("HindIII","NcoI")
            If excludeNeighbors is used, you have to specify restriction enzyme
        normalize : bool, optional
            Do an overall normalization of the answer, by default True.
        withinArms : bool, optional
            Set to false to use whole chromosomes instead of arms
        mindist, maxdist : int, optional
            Use lengthes from mindist to maxdist
        regions : list of (chrom,start,end) or (ch,st1,end1,st2,end2), optional
            Restrict scaling calculation to only certain squares of the map
        appendReadCount : bool, optional
            Append read count to the plot label
        plot : bool, optional
            If False then do not display the plot. True by default.
        **kwargs :  optional
            All other keyword args are passed to plt.plot

        Returns
        -------
        (bins,probabilities) - values to plot on the scaling plot

        """
        #TODO:(MI) write an ab-initio test for scaling calculation
        self._buildFragments()
        if excludeNeighbors <= 0:
            excludeNeighbors = None  # Not excluding neighbors

        #use all fragments if they're not specified
        #parse fragment array if it's bool
        if (fragids1 is None) and (fragids2 is None):
            allFragments = True
        else:
            allFragments = False
        if fragids1 is None:
            fragids1 = self.ufragments
        if fragids2 is None:
            fragids2 = self.ufragments
        if fragids1.dtype == np.bool:
            fragids1 = self.ufragments[fragids1]
        if fragids2.dtype == np.bool:
            fragids2 = self.ufragments[fragids2]

        #Calculate regions if not specified
        if regions is None:
            if withinArms == False:
                regions = [(i, 0, self.genome.chrmLens[i])
                    for i in xrange(self.genome.chrmCount)]
            else:
                regions = [(i, 0, self.genome.cntrMids[i])
                    for i in xrange(self.genome.chrmCount)] + \
                    [(i, self.genome.cntrMids[i], self.genome.chrmLens[i])
                    for i in xrange(self.genome.chrmCount)]

        if maxdist is None:
            maxdist = max(
                        max([i[2] - i[1] for i in regions]),
                        # rectangular regions
                        max([abs(i[2] - i[3]) for i in regions if
                             len(i) > 3] + [0]),
                        max([abs(i[1] - i[4]) for i in regions if
                             len(i) > 3] + [0])  # other side
                          )
        # Region to which a read belongs
        regionID = np.zeros(len(self.chrms1), np.int16) - 1
        chr1 = self.chrms1
        chr2 = self.chrms2
        pos1 = self.mids1
        pos2 = self.mids2
        fragRegions1 = np.zeros(len(fragids1), int) - 1
        fragRegions2 = np.zeros(len(fragids2), int) - 1
        fragch1 = fragids1 / self.fragIDmult
        fragch2 = fragids2 / self.fragIDmult
        fragpos1 = fragids1 % self.fragIDmult
        fragpos2 = fragids2 % self.fragIDmult

        for regionNum, region in enumerate(regions):
            if len(region) == 3:
                chrom, start1, end1 = region
                mask = (chr1 == chrom) * (pos1 > start1) * (pos1 < end1) * \
                (chr2 == chrom) * (pos2 > start1) * (pos2 < end1)
                regionID[mask] = regionNum
                mask1 = (fragch1 == chrom) * (fragpos1 >
                    start1) * (fragpos1 < end1)
                mask2 = (fragch2 == chrom) * (fragpos2 >
                    start1) * (fragpos2 < end1)
                fragRegions1[mask1] = regionNum
                fragRegions2[mask2] = regionNum

            if len(region) == 5:
                chrom, start1, end1, start2, end2 = region

                mask1 = (chr1 == chrom) * (chr2 == chrom) * (pos1 > start1) * \
                (pos1 < end1) * (pos2 > start2) * (pos2 < end2)

                mask2 = (chr1 == chrom) * (chr2 == chrom) * (pos1 > start2) * \
                (pos1 < end2) * (pos2 > start1) * (pos2 < end1)

                mask = mask1 + mask2
                regionID[mask] = regionNum
                mask1 = (fragch1 == chrom) * (
                    (fragpos1 > start1) * (fragpos1 < end1)
                    + (fragpos1 > start2) * (fragpos1 < end2))
                mask2 = (fragch2 == chrom) * (
                    (fragpos2 > start2) * (fragpos2 < end2)
                    + (fragpos2 > start1) * (fragpos2 < end1))
                fragRegions1[mask1] = regionNum
                fragRegions2[mask2] = regionNum
        del chr1, chr2, pos1, pos2

        bins = np.array(
            numutils.logbins(mindist, maxdist, 1.12), float) + 0.1  # bins of lengths
        numBins = len(bins) - 1  # number of bins

        #Keeping reads for fragments in use
        # Consider only double-sided fragment pairs.
        validFragPairs = self.DS
        if allFragments == False:
            # Filter the dataset so it has only the specified fragments.
            p11 = arrayInArray(self.fragids1, fragids1)
            p12 = arrayInArray(self.fragids1, fragids2)
            p21 = arrayInArray(self.fragids2, fragids1)
            p22 = arrayInArray(self.fragids2, fragids2)
            validFragPairs *= ((p11 * p22) + (p12 * p21))

        # Consider pairs of fragments from the same region.
        validFragPairs *= (regionID >= 0)
        # Keep only --> -->  or <-- <-- pairs, discard --> <-- and <-- -->
        validFragPairs *= (self.strands1 == self.strands2)

        # Keep only fragment pairs more than excludeNeighbors fragments apart.
        if excludeNeighbors is not None:
            if enzyme is None:
                raise ValueError("Please specify enzyme if you're"
                                 " excluding Neighbors")
            distsInFrags = self.genome.getFragmentDistance(
                self.fragids1, self.fragids2, enzyme)

            validFragPairs *= distsInFrags > excludeNeighbors

        distances = np.sort(self.distances[validFragPairs])

        "calculating fragments lengths for exclusions to expected # of counts"
        #sorted fragment IDs and lengthes
        args = np.argsort(self.ufragments)
        usort = self.ufragments[args]

        if useWeights == True:  # calculating weights if needed
            try:
                self.weights
            except:
                self.calculateWeights()
            uweights = self.weights[args]  # weights for sorted fragment IDs
            weights1 = uweights[np.searchsorted(usort, fragids1)]
            weights2 = uweights[np.searchsorted(usort, fragids2)
                ]  # weghts for fragment IDs under  consideration

        binBegs, binEnds = bins[:-1], bins[1:]

        numExpFrags = np.zeros(numBins)  # count of reads in each min
        fragpos1 = fragids1 % self.fragIDmult
        fragpos2 = fragids2 % self.fragIDmult

        for regionNumber, region in enumerate(regions):
            print region

            # filtering fragments that correspond to current region
            mask1 = np.nonzero(fragRegions1 == regionNumber)[0]
            mask2 = np.nonzero(fragRegions2 == regionNumber)[0]

            if (len(mask1) == 0) or (len(mask2) == 0):
                continue
            bp1, bp2 = fragpos1[mask1], fragpos2[mask2]
                #positions of fragments on chromosome

            p2arg = np.argsort(bp2)
            p2 = bp2[p2arg]  # sorted positions on the second fragment

            if excludeNeighbors is not None:
                "calculating excluded fragments (neighbors) and their weights"\
                " to subtract them later"
                excFrag1, excFrag2 = self.genome.getPairsLessThanDistance(
                    fragids1[mask1], fragids2[mask2], excludeNeighbors, enzyme)
                excDists = np.abs(excFrag2 - excFrag1)
                    #distances between excluded fragment pairs
                if useWeights == True:
                    correctionWeights = weights1[numutils.arraySearch(
                        fragids1, excFrag1)]

                    # weights for excluded fragment pairs
                    correctionWeights = correctionWeights * weights2[
                                numutils.arraySearch(fragids2, excFrag2)]
            if useWeights == True:
                w1, w2 = weights1[mask1], weights2[mask2]
                sw2 = np.r_[0, np.cumsum(w2[p2arg])]
                    #cumsum for sorted weights on 2 strand

            for minDist, maxDist, binIndex in zip(binBegs, binEnds, range(numBins)):
                "Now calculating actual number of fragment pairs for a "\
                "length-bin, or weight of all these pairs"

                # For each first fragment in a pair, calculate total # of
                # restriction fragments in the genome lying downstream within
                # the bin.
                val1 = np.searchsorted(p2, bp1 - maxDist)
                val2 = np.searchsorted(p2, bp1 - minDist)
                if useWeights == False:
                    curcount = np.sum(np.abs(val1 - val2))  # just # of fragments
                else:
                    # (difference in cumsum of weights) * my weight
                    curcount = np.sum(w1 * np.abs(sw2[val1] - sw2[val2]))

                # Repeat the procedure for the fragments lying upstream.
                val1 = np.searchsorted(p2, bp1 + maxDist)
                val2 = np.searchsorted(p2, bp1 + minDist)
                if useWeights == False:
                    curcount += np.sum(np.abs(val1 - val2))
                else:
                    curcount += np.sum(w1 * np.abs(sw2[val1] - sw2[val2]))

                # now modifying expected count because of excluded fragments
                if excludeNeighbors is not None:
                    if useWeights == False:
                        ignore = ((excDists > minDist) *
                            (excDists < maxDist)).sum()
                    else:
                        ignore = (correctionWeights[((excDists > minDist) * \
                                                (excDists < maxDist))]).sum()

                    if (ignore >= curcount) and (ignore != 0):
                        if ignore < curcount * 1.0001:
                            curcount = ignore = 0
                        else:
                            print "error found", "minDist:", minDist
                            print "  curcount:", curcount, "  ignore:", ignore
                    else:  # Everything is all right
                        curcount -= ignore
                numExpFrags[binIndex] += curcount

        values = []
        rawValues = []
        binBegs, binEnds = bins[:-1], bins[1:]
        binMids = 0.5 * (binBegs + binEnds).astype(float)
        binLens = binEnds - binBegs

        for i in xrange(len(bins) - 1):  # Dividing observed by expected
            first, last = tuple(np.searchsorted(distances, [binBegs[i], binEnds[i]]))
            mycounts = last - first
            values.append(mycounts / float(numExpFrags[i]))
            rawValues.append(mycounts)

        values = np.array(values)

        if normalize == True:
            if normRange is None:
                values /= np.sum(
                    1. * (binLens * values)[
                        np.logical_not(
                            np.isnan(binMids * values))])
            else:
                values /= np.sum(
                    1. * (binLens * values)[
                        np.logical_not(
                            np.isnan(binMids * values))
                            * (binMids > normRange[0])
                            * (binMids < normRange[1])])

        do_plot = kwargs.pop('plot', True)
        if do_plot:
            if appendReadCount == True:
                if "label" in kwargs.keys():
                    kwargs["label"] = kwargs["label"] + \
                        ", %d reads" % len(distances)
            plt.plot(binMids, values, **kwargs)
        return (binMids, values)

    def plotRsiteStartDistribution(self, useSSReadsOnly=False,
                                   offset=5, length=200):
        """
        run plt.show() after this function.
        """
        if useSSReadsOnly == True:
            mask = self.SS
        else:
            mask = self.DS
        dists1 = self.fraglens1 - np.array(self.dists1, dtype="int32")
        dists2 = self.fraglens2 - np.array(self.dists2, dtype="int32")
        m = min(dists1.min(), dists2.min())
        if offset < -m:
            offset = -m
            print "minimum negative distance is %d, larger than offset;"\
            " offset set to %d" % (m, -m)
        dists1 += offset
        dists2 += offset
        myrange = np.arange(-offset, length - offset)

        plt.subplot(141)
        plt.title("strands1, side 1")
        plt.plot(myrange, np.bincount(
            5 + dists1[mask][self.strands1[mask] == True])[:length])
        plt.subplot(142)
        plt.title("strands1, side 2")
        plt.plot(myrange, np.bincount(
            dists2[mask][self.strands1[mask] == True])[:length])

        plt.subplot(143)
        plt.title("strands2, side 1")
        plt.plot(myrange, np.bincount(
            dists1[mask][self.strands1[mask] == False])[:length])
        plt.subplot(144)
        plt.title("strands2, side 2")
        plt.plot(myrange, np.bincount(
            dists2[mask][self.strands1[mask] == False])[:length])

class experimentalFeatures(HiCdataset):
    "This class contain some dangerous features that were not tested."
    def splitFragmentsBystrands(self):
        """Splits fragments: those with strands = 1 gets location += 1.
        This is totally safe!
        This might be fun if you want to analyze two sides of the
        fragment separately, but unnecessary otherwise"""
        f1 = self.fragids1
        f1 += ((f1 % self.fragIDmult) % 2)
        f1[self.strands1 == 1] += 1
        f2 = self.fragids2
        f1 += ((f2 % self.fragIDmult) % 2)
        f2[self.strands1 == 1] += 1
        self.rebuildFragments()
