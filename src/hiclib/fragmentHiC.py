#(c) 2012 Massachusetts Institute of Technology. All Rights Reserved
# Code written by: Maksim Imakaev (imakaev@mit.edu)
"""
This is a module class for fragment-level Hi-C data analysis.
The base class "HiCdataset" can load, save and merge Hi-C datasets,
perform certain filters, and save binned heatmaps.

Additional class HiCStatistics contains methods to analyze HiC data
on a fragment level.
This includes read statistics, scalings, etc.

Input data
----------

When used together with iterative mapping, this class can load
files from h5dicts created by iterative mapping.

This method can also input any dictionary-like structure, such as a dictionary,
np.savez, etc. The minimal subset of information are positions of two reads,
but providing strand informations is adviced.

If restriction fragment assignment is not provided,
it will be automatically recalculated.

.. warning ::
    1-bp difference in positions of restriction sites will force certain
    algorithms, such as scaling calculations, to throw an exception. It is
    adviced to supply restriction site data only if it was generated by
    iterative mapping code.

Concepts
--------

All read data is stored in a synchronized h5dict-based dictionary of arrays.
Each variable has a fixed name and type, as specified in the self.vectors
variable. Whenever the variable is accessed from the program, it is loaded from
the h5dict.

All fragment data is stored in RAM, and can be re-created at any point using
:py:func:`rebuildFragments <HiCdataset.rebuildFragments>`

Whenever a set of reads needs to be excluded from the dataset, a
:py:func:`maskFilter  <HiCdataset.maskFilter>` method is called,
that goes over all datasets and overrides them.
This method automatically rebuilds fragments.

Filtering the data
------------------

This class has many build-in methods for filtering the data.
However, one can easily construct another filter as presented in
multiple one-liner examples below

.. code-block:: python

    >>> Dset = HiCdataset(**kwargs)
    >>> Dset.fragmentFilter((Dset.ufragmentlen >1000) * \
    (Dset.ufragmentlen < 4000))
    #Keep reads from fragments between 1kb and 4kb long.

    >>> Dset.maskFilter(Dset.chrms1 == Dset.chrms2)  #keep only cis reads

    >>> Dset.maskFilter((Dset.chrms1 !=14) + (Dset.chrms2 !=14))
    #Exclude all reads from chromosome 15 (yes, chromosomes are zero-based!)

    >>> Dset.maskFilter(Dset.dist1 + Dset.dist2 > 500)
    #Keep only random breaks, if 500 is maximum molecule length

-------------------------------------------------------------------------------

API documentation
-----------------



"""

import warnings
import os
import traceback
from copy import copy
from mirnylib.genome import Genome
import sys
import hiclib.mapping
import numpy as np
import math
from numpy import array as na
from scipy import stats
from mirnylib.h5dict import h5dict
from mirnylib.plotting import mat_img
from mirnylib import numutils
from mirnylib.numutils import arrayInArray, sumByArray, \
    uniqueIndex, chunkedUnique, fasterBooleanIndexing, fillDiagonal, arraySearch, \
    arraySumByArray
import time

r_ = np.r_


def corr(x, y):
    return stats.spearmanr(x, y)[0]


class HiCdataset(object):
    """Base class to operate on HiC dataset.

    .. warning ::
        This is the last build to support SS reads; from now on
        SS reads will be deprecated as noone uses them :(

    This class stores all information about HiC reads on a hard drive.
    Whenever a variable corresponding to any record is used,
    it is loaded/saved from/to the HDD.

    If you apply any filters to a dataset, it will actually modify
    the content of the current working file.
    Thus, to preserve the data, loading datasets is advised. """

    def __init__(self, filename, genome, maximumMoleculeLength=500,
                 override="deprecated", autoFlush="deprecated",
                 inMemory=False, mode="a"):
        """
        __init__ method

        Initializes empty dataset by default.
        If "override" is False, works with existing dataset.

        Parameters
        ----------
        filename : string
            A filename to store HiC dataset in an HDF5 file.
        genome : folder with genome, or Genome object
            A folder with fastq files of the genome
            and gap table from Genome browser.
            Alternatively, mirnylib.genome.Genome object.
        maximumMoleculeLength : int, optional
            Maximum length of molecules in the HiC library,
            used as a cutoff for dangling ends filter
        override : bool, optional, deprecated
            If true, file will be overwritten. Deprecated,
            use "mode = 'w'" instead of "override = True".
        autoFlush : bool, optional, deprecated
            Set to True to disable autoflush -
            possibly speeds up read/write operations.
            Currently deprecated.
        inMemory : bool, optional
            Create dataset in memory. Filename is ignored then,
            but still needs to be specified.
        mode : str
            'r'  - Readonly, file must exist

            'r+' - Read/write, file must exist

            'w'  - Create file, overwrite if exists

            'w-' - Create file, fail if exists

            'a'  - Read/write if exists, create otherwise (default)
        """
        #-->>> Important::: do not define any variables before vectors!!! <<<--
        #These are fields that will be kept on a hard drive
        #You can learn what variables mean from here too.
        self.vectors = {
            # chromosomes for each read.
            "chrms1": "int8", "chrms2": "int8",

            "mids1": "int32", "mids2": "int32",
            # midpoint of a fragment, determined as "(start+end)/2"

            "fraglens1": "int32", "fraglens2": "int32",
            # fragment lengthes

            "distances": "int32",
            # distance between fragments. If -1, different chromosomes.
            #If -2, different arms.

            "fragids1": "int64", "fragids2": "int64",
            # IDs of fragments. fragIDmult * chromosome + location
            # distance to rsite
            "dists1": "int32", "dists2": "int32",
            # precise location of cut-site
            "cuts1": "int32", "cuts2": "int32",
            "strands1": "bool", "strands2": "bool",
            }
        self.metadata = {}
            #'rfragAbsIdxs1':'int32',
            #'rfragAbsIdxs2':'int32'}

        #--------Deprecation warnings-------
        if override != "deprecated":
            warnings.warn(UserWarning("Please use a more intuitive flag "
                                      "'mode =' instead of 'override ='"))
            mode = 'w'
        elif override == True:
            mode = "w"
        if autoFlush != "deprecated":
            warnings.warn(UserWarning("Autoflush was deprecated, "
                                      "inMemory is adviced instead "
                                      "when memory is not an issue"))

        #-------Initialization of the genome and parameters-----
        self.mode = mode
        if type(genome) == str:
            self.genome = Genome(genomePath=genome, readChrms=["#", "X"])
        else:
            self.genome = genome
        assert isinstance(self.genome, Genome)  # bla

        self.chromosomeCount = self.genome.chrmCount
        self.fragIDmult = self.genome.fragIDmult  # used for building heatmaps
        print "----> New dataset opened, genome %s, filename = %s" % (
            self.genome.folderName, filename)

        self.maximumMoleculeLength = maximumMoleculeLength
        # maximum length of a molecule for SS reads

        self.filename = os.path.abspath(os.path.expanduser(filename))  # File to save the data
        self.chunksize = 5000000
        # Chunk size for h5dict operation, external sorting, etc.

        self.inMemory = inMemory

        #------Creating filenames, etc---------
        if os.path.exists(self.filename) and (mode in ['w', 'a']):
            print '----->!!!File already exists! It will be {0}\n'.format(
                {"w": "deleted", "a": "opened in the append mode"}[mode])

        if len(os.path.split(self.filename)[0]) != 0:
            if not os.path.exists(os.path.split(self.filename)[0]):
                warnings.warn("Folder in which you want to create file"
                        "do not exist: %s" % os.path.split(self.filename)[0])
                try:
                    os.mkdir(os.path.split(self.filename)[0])
                except:
                    raise IOError("Failed to create directory: %s" %
                                  os.path.split(self.filename)[0])

        self.h5dict = h5dict(self.filename, mode=mode, in_memory=inMemory)
        if "chrms1" in self.h5dict.keys():
            chrms1 = self.chrms2
            self.DSnum = self.N = len(chrms1)



    def _setData(self, name, data):
        "an internal method to save numpy arrays to HDD quickly"
        if name not in self.vectors.keys():
            raise ValueError("Attept to save data not "
                             "specified in self.vectors")
        dtype = np.dtype(self.vectors[name])
        data = np.asarray(data, dtype=dtype)
        self.h5dict[name] = data

    def _getData(self, name):
        "an internal method to load numpy arrays from HDD quickly"
        if name not in self.vectors.keys():
            raise ValueError("Attept to load data not "
                             "specified in self.vectors")
        return self.h5dict[name]

    def __getattribute__(self, x):
        """a method that overrides set/get operation for self.vectors
        o that they're always on HDD"""
        if x == "vectors":
            return object.__getattribute__(self, x)

        if x in self.vectors.keys():
            a = self._getData(x)
            return a
        else:
            return object.__getattribute__(self, x)

    def __setattr__(self, x, value):
        """a method that overrides set/get operation for self.vectors
        so that they're always on HDD"""
        if x == "vectors":
            return object.__setattr__(self, x, value)

        if x in self.vectors.keys():
            self._setData(x, value)
        else:
            return object.__setattr__(self, x, value)

    def _buildFragments(self):
        if not hasattr(self, "ufragments"):
            self.rebuildFragments()

    def _dumpMetadata(self):

        if self.mode in ["r"]:
            warnings.warn(RuntimeWarning("Cannot dump metadata in read mode"))
            return

        try:
            self.h5dict["metadata"] = self.metadata

        except Exception, err:
            print "-" * 20 + "Got Exception when saving metadata" + "-" * 20
            traceback.print_exc()
            print Exception, err
            print "-" * 60
            warnings.warn(RuntimeWarning("Got exception when saving metadata"))


    def evaluate(self, expression, internalVariables, externalVariables={},
                 constants={"np": np},
                 outVariable="autodetect",
                 chunkSize="default"):
        """
        Still experimental class to perform evaluation of
        any expression on hdf5 datasets
        Note that out_variable should be writable by slices.

        ---If one can provide autodetect of values for internal
        variables by parsing an expression, it would be great!---

        .. note ::
            See example of usage of this class in filterRsiteStart,
            parseInputData, etc.

        .. warning ::
            Please avoid passing internal variables
            as "self.cuts1" - use "cuts1"

        .. warning ::
            You have to pass all the modules and functions (e.g. np)
            in a "constants" dictionary.

        Parameters
        ----------
        expression : str
            Mathematical expression, single or multi line
        internal_variables : list of str
            List of variables ("chrms1", etc.), used in the expression
        external_variables : dict , optional
            Dict of {str:array}, where str indicates name of the variable,
            and array - value of the variable.
        constants : dict, optional
            Dictionary of constants to be used in the evaluation.
            Because evaluation happens without namespace,
            you should include numpy here if you use it (included by default)
        out_variable : str or tuple or None, optional
            Variable to output the data. Either internal variable, or tuple
            (name,value), where value is an array
        """
        if type(internalVariables) == str:
            internalVariables = [internalVariables]
        if chunkSize == "default":
            chunkSize = self.chunksize

        # detecting output variable automatically
        if outVariable == "autodetect":
            outVariable = expression.split("\n")[-1].split("=")[0].strip()
            if outVariable not in self.vectors:
                outVariable = (outVariable, "ToDefine")

        code = compile(expression, '<string>', 'exec')
            #compile because we're launching it many times

        bins = range(0, self.N, chunkSize) + [self.N]
            #creating bins to perform evaluation
        bins = zip(bins[:-1], bins[1:])
        for start, end in bins:

            variables = copy(constants)
            # dictionary to pass to the evaluator.
            #It's safer than to use the default locals()

            for name in internalVariables:
                if name not in self.h5dict.keys():
                    raise ValueError("{0} not in keys".format(name))
                variables[name] = self.h5dict.get_dataset(name)[start:end]

            for name, variable in externalVariables.items():
                variables[name] = variable[start:end]

            # actually execute the code in our own namespace
            exec code in variables

            # autodetecting output dtype on the first run if not specified
            if outVariable[1] == "ToDefine":
                dtype = variables[outVariable[0]].dtype
                outVariable = (outVariable[0], np.zeros(self.N, dtype))

            if type(outVariable) == str:
                self.h5dict.get_dataset(outVariable)[start:end] = variables[outVariable]
            elif len(outVariable) == 2:
                outVariable[1][start:end] = variables[outVariable[0]]
            elif outVariable is None:
                pass
            else:
                raise ValueError("Please provide str or (str,value)"
                                 " for out variable")

        if type(outVariable) == tuple:
            return outVariable[1]

    def flush(self):
        warnings.warn(UserWarning(
            "Autoflush was deprecated, flush is useless now"))

    def merge(self, filenames):
        """combines data from multiple datasets

        Parameters
        ----------
            filenames : list of strings
                List of folders to merge to current working folder
        """
        if self.filename in filenames:
            raise StandardError("----> Cannot merge folder into itself! "
                                "Create a new folder")
        for filename in filenames:
            if not os.path.exists(filename):
                raise IOError("\nCannot open file: %s" % filename)

        h5dicts = [h5dict(i, mode='r') for i in filenames]
        if all(["metadata" in i for i in h5dicts]):
            metadatas = [mydict["metadata"] for mydict in h5dicts]
            #print metadatas
            newMetadata = metadatas.pop()
            for oldData in metadatas:
                for key, value in oldData.items():
                    if (key in newMetadata):
                        try:
                            newMetadata[key] += value
                        except:
                            print "Values {0} and {1} for key {2} cannot be added".format(metadatas[key], value, key)
                            warnings.warn("Cannot add metadatas")
                    else:
                        warnings.warn("key {0} not found in some files".format(key))
            self.metadata = newMetadata
            self.h5dict["metadata"] = self.metadata

        for name in self.vectors.keys():
            res = []
            for mydict in h5dicts:
                res.append(mydict[name])
            res = np.concatenate(res)
            self.N = len(res)
            self.DSnum = self.N
            self._setData(name, res)
            self.h5dict.flush()
            time.sleep(0.2)  # allow buffers to flush


        self.rebuildFragments()

    def parseInputData(self, dictLike, zeroBaseChrom=True,
                       enzymeToFillRsites=None, **kwargs):
        """Inputs data from a dictionary-like object,
        containing coordinates of the reads.
        Performs filtering of the reads.

        A good example of a dict-like object is a numpy.savez

        .. warning::
            Restriction fragments MUST be specified
            exactly as in the Genome class.

        .. warning::
            Strand information is needed for proper scaling
            calculations, but will be imitated if not provided

        Parameters
        ----------
        dictLike : dict or dictLike object, or string with h5dict filename
            Input reads

        dictLike["chrms1,2"] : array-like
            Chromosomes of 2 sides of the read
        dictLike["cuts1,2"] :  array-like
            Exact position of cuts
        dictLike["strands1,2"], essential : array-like
            Direction of the read
        dictLike["rsites1,2"], optional : array-like
            Position of rsite to which the read is pointing
        dictLike["uprsites1,2"] , optional : array-like
            rsite upstream (larger genomic coordinate) of the cut position
        dictLike["downrsites1,2"] , optional : array-like
            rsite downstream (smaller genomic coordinate) of the cut position

        zeroBaseChrom : bool , optional
            Use zero-base chromosome counting if True, one-base if False
        enzymeToFillRsites : None or str, optional if rsites are specified
            Enzyme name to use with Bio.restriction
        removeSS : bool, optional
            If set to True, removes SS reads from the library
        noFiltering : bool, optional
            If True then no filters are applied to the data. False by default.
            Overrides removeSS. Experimental, do not use if you are not sure.
        """


        rsite_related = ["rsites1", "rsites2", "uprsites1",
                         "uprsites2", "downrsites1", "downrsites2"]
        if type(dictLike) == str:
            if not os.path.exists(dictLike):
                raise IOError("File not found: %s" % dictLike)
            print "     loading data from file %s (assuming h5dict)" % dictLike
            dictLike = h5dict(dictLike, 'r')  # attempting to open h5dict

        if all([i in dictLike.keys() for i in rsite_related]):
            noRsites = False
        else:
            noRsites = True

        "---Filling in chromosomes and positions - mandatory objects---"
        a = dictLike["chrms1"]
        self.trackLen = len(a)

        if zeroBaseChrom == True:
            self.chrms1 = a
            self.chrms2 = dictLike["chrms2"]
        else:
            self.chrms1 = a - 1
            self.chrms2 = dictLike["chrms2"] - 1
        self.N = len(self.chrms1)
        del a

        self.cuts1 = dictLike['cuts1']
        self.cuts2 = dictLike['cuts2']

        if not (("strands1" in dictLike.keys()) and
                ("strands2" in dictLike.keys())):
            warnings.warn("No strand information provided,"
                          " assigning random strands.")
            t = np.random.randint(0, 2, self.trackLen)
            self.strands1 = t
            self.strands2 = 1 - t
            del t
            noStrand = True
        else:
            self.strands1 = dictLike["strands1"]
            self.strands2 = dictLike["strands2"]
            noStrand = False  # strand information filled in

        # We have to fill rsites ousrlves. Let's see what enzyme to use!
        if noRsites == True:
            if ((enzymeToFillRsites is None) and
                (self.genome.hasEnzyme() == False)):
                raise ValueError("Please specify enzyme"
                                 " if your data has no rsites")

            if enzymeToFillRsites is not None:
                if self.genome.hasEnzyme() == True:
                    if enzymeToFillRsites != self.genome.enzymeName:
                        warnings.warn("genome.enzymeName "
                                      "different from supplied enzyme")
                self.genome.setEnzyme(enzymeToFillRsites)
            #enzymeToFillRsites has preference over self.genome's enzyme

            print "Filling rsites"
            rsitedict = h5dict(in_memory=self.inMemory)
                #creating dict to pass to fillRsite's code
            rsitedict["chrms1"] = self.chrms1
            rsitedict["chrms2"] = self.chrms2
            rsitedict["cuts1"] = self.cuts1
            rsitedict["cuts2"] = self.cuts2
            rsitedict["strands1"] = self.strands1
            rsitedict["strands2"] = self.strands2
            hiclib.mapping.fill_rsites(
                lib=rsitedict, genome_db=self.genome)
        else:
            rsitedict = dictLike  # rsite information is in our dictionary

        "---Now filling all actual values"
        self.dists1 = np.abs(rsitedict["rsites1"] - self.cuts1)
        self.dists2 = np.abs(rsitedict["rsites2"] - self.cuts2)

        self.mids1 = (rsitedict["uprsites1"] + rsitedict["downrsites1"]) / 2
        self.mids2 = (rsitedict["uprsites2"] + rsitedict["downrsites2"]) / 2

        self.fraglens1 = np.abs(
            (rsitedict["uprsites1"] - rsitedict["downrsites1"]))
        self.fraglens2 = np.abs(
            (rsitedict["uprsites2"] - rsitedict["downrsites2"]))

        del rsitedict  # deletes hdf5 file, so it's important

        self.fragids1 = self.mids1 + np.array(self.chrms1,
                                              dtype="int64") * self.fragIDmult
        self.fragids2 = self.mids2 + np.array(self.chrms2,
                                              dtype="int64") * self.fragIDmult

        distances = np.abs(self.mids1 - self.mids2)
        distances[self.chrms1 != self.chrms2] = -1
        self.distances = distances  # distances between restriction fragments
        del distances

        self.metadata["100_TotalReads"] = self.trackLen

        try:
            dictLike["misc"]["genome"]["idx2label"]
            self.updateGenome(self.genome, oldGenome=dictLike["misc"]["genome"]["idx2label"], putMetadata=True)

        except KeyError:
            assumedGenome = Genome(self.genome.genomePath)
            self.updateGenome(self.genome, oldGenome=assumedGenome, putMetadata=True)
            warnings.warn("\n Genome not found in mapped data. \n"
            "Assuming genome comes from the same folder with all chromosomes")

        self.metadata["152_removedUnusedChromosomes"] = self.trackLen - self.N
        self.metadata["150_ReadsWithoutUnusedChromosomes"] = self.N

        # Discard dangling ends and self-circles
        DSmask = (self.chrms1 >= 0) * (self.chrms2 >= 0)
        self.metadata["200_totalDSReads"] = DSmask.sum()
        self.metadata["201_DS+SS"] = len(DSmask)
        self.metadata["202_SSReadsRemoved"] = len(DSmask) - DSmask.sum()

        sameFragMask = self.evaluate("a = (fragids1 == fragids2)",
                     ["fragids1", "fragids2"]) * DSmask

        cutDifs = self.cuts2[sameFragMask] > self.cuts1[sameFragMask]
        s1 = self.strands1[sameFragMask]
        s2 = self.strands2[sameFragMask]
        SSDE = (s1 != s2)
        SS = SSDE * (cutDifs == s2)
        SS_N = SS.sum()
        SSDE_N = SSDE.sum()
        sameFrag_N = sameFragMask.sum()
        self.metadata["210_sameFragmentReadsRemoved"] = sameFrag_N
        self.metadata["212_Self-Circles"] = SS_N
        self.metadata["214_DandlingEnds"] = SSDE_N - SS_N
        self.metadata["216_error"] = sameFrag_N - SSDE_N

        mask = DSmask * (-sameFragMask)
        del DSmask, sameFragMask
        noSameFrag = mask.sum()

        # Discard unused chromosomes

        if noStrand == True:
            # Can't tell if reads point to each other.
            dist = self.evaluate("a = np.abs(cuts1 - cuts2)",
                                 ["cuts1", "cuts2"])
        else:
            # distance between sites facing each other
            dist = self.evaluate("a = - cuts1 * (2 * strands1 -1) - "
                                 "cuts2 * (2 * strands2 - 1)",
                                 ["cuts1", "cuts2", "strands1", "strands2"])

        readsMolecules = self.evaluate(
           "a = (chrms1 == chrms2)*(strands1 != strands2) *  (dist >=0) *"
           " (dist <= maximumMoleculeLength)",
           internalVariables=["chrms1", "chrms2", "strands1", "strands2"],
           externalVariables={"dist": dist},
           constants={"maximumMoleculeLength": self.maximumMoleculeLength})
        mask *= (readsMolecules == False)
        extraDE = mask.sum()
        self.metadata["220_extraDandlingEndsRemoved"] = -extraDE + noSameFrag
        if mask.sum() == 0:
            raise Exception(
                'No reads left after filtering. Please, check the input data')

        del dist
        del readsMolecules
        if not kwargs.get('noFiltering', False):
            self.maskFilter(mask)
        self.metadata["300_ValidPairs"] = self.N

    def printMetadata(self, saveTo=None):
        self._dumpMetadata()
        for i in sorted(self.metadata):
            if i[2] != "0":
                print "\t\t",
            elif i[1] != "0":
                print "\t",
            print i, self.metadata[i]
        if saveTo != None:
            with open(saveTo, 'w') as myfile:
                for i in sorted(self.metadata):
                    if i[2] != "0":
                        myfile.write("\t\t")
                    elif i[1] != "0":
                        myfile.write("\t")
                    myfile.write(str(i))
                    myfile.write(":   ")
                    myfile.write(str(self.metadata[i]))
                    myfile.write("\n")

    def saveFragments(self):
        """saves fragment data to make correct expected
        estimates after applying a heavy mask"""
        self.ufragmentsOriginal = np.array(self.ufragments)
        self.ufragmentlenOriginal = np.array(self.ufragmentlen)

    def originalFragments(self):
        "loads original fragments"
        self.ufragments = np.array(self.ufragmentsOriginal)
        self.ufragmentlen = np.array(self.ufragmentlenOriginal)

    def updateGenome(self, newGenome, removeSSreads="deprecated",
                     oldGenome="current", putMetadata=False):
        """
        Updates dataset to a new genome, with a fewer number of chromosomes.
        Use it to delete chromosomes.
        By default, removes all DS reads with that chromosomes.

        Parameters
        ----------
        newGenome : Genome object
            Genome to replace the old genome, with fewer chromosomes
        removeSSreads : "trans"(default), "all" or "none"
            "trans": remove all reads from deleted chromosomes,
            ignore the rest.
            "all": remove all SS reads from all chromosomes
            "None": mark all trans reads as SS reads
        putMetadata : bool (optional)
            Writes metadata for M and Y reads

        oldGenome : Genome object or idx2label of old genome, optional

        """

        if removeSSreads != "deprecated":
            DeprecationWarning("SS reads are deprecated now! Modify your code")

        assert isinstance(newGenome, Genome)
        newN = newGenome.chrmCount
        if oldGenome == "current":
            oldGenome = self.genome
        upgrade = newGenome.upgradeMatrix(oldGenome)
        if isinstance(oldGenome, Genome):
            if oldGenome.hasEnzyme():
                newGenome.setEnzyme(oldGenome.enzymeName)
            oldGenome = oldGenome.idx2label
        oldN = len(oldGenome.keys())
        label2idx = dict(zip(oldGenome.values(), oldGenome.keys()))
        chrms1 = np.array(self.chrms1, int)
        chrms2 = np.array(self.chrms2, int)
        SS = (chrms1 < 0) + (chrms2 < 0)
        metadata = {}
        if "M" in label2idx:
            Midx = label2idx["M"]
            M1 = chrms1 == Midx
            M2 = chrms2 == Midx
            mToM = (M1 * M2).sum()
            mToAny = (M1 + M2).sum()
            mToSS = ((M1 + M2) * SS).sum()
            metadata["102_mappedSide1"] = (chrms1 >= 0).sum()
            metadata["104_mappedSide2"] = (chrms2 >= 0).sum()

            metadata["112_M-to-M_reads"] = mToM
            metadata["114_M-to-Any_reads"] = mToAny
            metadata["116_M-to-SS_reads"] = mToSS
            metadata["118_M-to-DS_reads"] = mToAny - mToSS

        if "Y" in label2idx:
            Yidx = label2idx["Y"]
            Y1 = chrms1 == Yidx
            Y2 = chrms2 == Yidx
            yToY = (Y1 * Y2).sum()
            yToAny = (Y1 + Y2).sum()
            yToSS = ((Y1 + Y2) * SS).sum()

            metadata["122_Y-to-Y_reads"] = yToY
            metadata["124_Y-to-Any_reads"] = yToAny
            metadata["126_Y-to-SS_reads"] = yToSS
            metadata["128_Y-to-DS_reads"] = yToAny - yToSS

        if putMetadata:
            self.metadata.update(metadata)

        if oldN == newN:
            return None

        if upgrade is not None:
            upgrade[upgrade == -1] = 9999  # to tell old SS reads from new SS reads

            chrms1 = upgrade[chrms1]
            self.chrms1 = chrms1
            del chrms1

            chrms2 = upgrade[chrms2]
            self.chrms2 = chrms2

        "Keeping only DS reads"
        mask = ((self.chrms1 < newN) * (self.chrms2 < newN))
        self.genome = newGenome
        self.maskFilter(mask)

    def calculateFragmentWeights(self):
        """Calculates weights for reads based on fragment length correction
         similar to Tanay's;
         may be used for scalings or creating heatmaps"""
        self._buildFragments()
        fragmentLength = self.ufragmentlen
        pls = np.sort(fragmentLength)
        pls = np.r_[pls, pls[-1] + 1]
        N = len(fragmentLength)
        mysum = np.array(self.fragmentSum(), float)
        self.fragmentWeights = np.ones(N, float)
        meanSum = np.mean(np.array(mysum, float))
        #watch = np.zeros(len(mysum),int)
        for i in np.arange(0, 0.991, 0.01):
            b1, b2 = pls[i * N], pls[(i + 0.01) * N]
            p = (b1 <= fragmentLength) * (b2 > fragmentLength)
            #watch[p] += 1
            value = np.mean(mysum[p])
            if p.sum() > 0:
                self.fragmentWeights[p] = value / meanSum
            else:
                print "no weights", i, b1, b2

    def buildAllHeatmap(self, resolution, countDiagonalReads="Once",
        useWeights=False):
        """Creates an all-by-all heatmap in accordance with mapping
        provided by 'genome' class

        Parameters
        ----------
        resolution : int or str
            Resolution of a heatmap. May be an int or 'fragment' for
            restriction fragment resolution.
        countDiagonalReads : "once" or "twice"
            How many times to count reads in the diagonal bin
        useWeights : bool
            If True, then take weights from 'weights' variable. False by default.
        """
        if type(resolution) == int:
            #8 bytes per record + heatmap
            self.genome.setResolution(resolution)
            numBins = self.genome.numBins
            label = self.genome.chrmStartsBinCont[self.chrms1]
            label = np.asarray(label, dtype="int64")
            label += self.mids1 / resolution
            label *= numBins
            label += self.genome.chrmStartsBinCont[self.chrms2]
            label += self.mids2 / resolution

        elif resolution == 'fragment':
            numBins = self.genome.numRfrags
            label = self.rfragAbsIdxs1
            label *= numBins
            label += self.rfragAbsIdxs2
        else:
            raise Exception('Unknown value for resolution: {0}'.format(
                resolution))

        if useWeights:
            if 'weights' not in self.vectors:
                raise Exception('Set read weights first!')
            counts = np.bincount(label, weights=self.fragmentWeights, minlength=numBins ** 2)
        else:
            counts = np.bincount(label, minlength=numBins ** 2)
        if len(counts) > numBins ** 2:
            raise StandardError("\nheatmap exceed length of the genome!!!"
                                " Check genome")

        counts.shape = (numBins, numBins)
        for i in xrange(len(counts)):
            counts[i, i:] += counts[i:, i]
            counts[i:, i] = counts[i, i:]
        if countDiagonalReads.lower() == "once":
            diag = np.diag(counts)
            fillDiagonal(counts, diag / 2)
        elif countDiagonalReads.lower() == "twice":
            pass
        else:
            raise ValueError("Bad value for countDiagonalReads")
        return counts


    def buildHeatmapWithOverlapCpp(self, resolution, countDiagonalReads="Twice",
        maxBinSpawn=10):
        """Creates an all-by-all heatmap in accordance with mapping
        provided by 'genome' class

        This method assigns fragments to all bins which
        the fragment overlaps, proportionally

        Parameters
        ----------
        resolution : int or str
            Resolution of a heatmap. May be an int or 'fragment' for
            restriction fragment resolution.
        countDiagonalReads : "once" or "twice"
            How many times to count reads in the diagonal bin
        maxBinSpawn : int, optional, not more than 10
            Discard read if it spawns more than maxBinSpawn bins

        """


        if type(resolution) == int:

            #many bytes per record + heatmap
            self.genome.setResolution(resolution)
            N = self.N
            N = int(N)

            low1 = self.genome.chrmStartsBinCont[self.chrms1]
            low1 = np.asarray(low1, dtype="float32")
            low1 += (self.mids1 - self.fraglens1 / 2) / float(resolution)

            high1 = self.genome.chrmStartsBinCont[self.chrms1]
            high1 = np.asarray(high1, dtype="float32")
            high1 += (self.mids1 + self.fraglens1 / 2) / float(resolution)

            low2 = self.genome.chrmStartsBinCont[self.chrms2]
            low2 = np.asarray(low2, dtype="float32")
            low2 += (self.mids2 - self.fraglens2 / 2) / float(resolution)

            high2 = self.genome.chrmStartsBinCont[self.chrms2]
            high2 = np.asarray(high2, dtype="float32")
            high2 += (self.mids2 + self.fraglens2 / 2) / float(resolution)


            heatmap = np.zeros((self.genome.numBins, self.genome.numBins),
                               dtype="float64", order="C")
            heatmapSize = len(heatmap)


            from scipy import weave
            code = """
            #line 1045 "fragmentHiC.py"
            double vector1[100];
            double vector2[100];

            for (int readNum = 0;  readNum < N; readNum++)
            {
                for (int i=0; i<10; i++)
                {
                    vector1[i] = 0;
                    vector2[i] = 0;
                }

                double l1 = low1[readNum];
                double l2 = low2[readNum];
                double h1 = high1[readNum];
                double h2 = high2[readNum];


                if ((h1 - l1) > maxBinSpawn) continue;
                if ((h2 - l2) > maxBinSpawn) continue;

                int binNum1 = ceil(h1) - floor(l1);
                int binNum2 = ceil(h2) - floor(l2);
                double binLen1 = h1 - l1;
                double binLen2 = h2 - l2;

                int b1 = floor(l1);
                int b2 = floor(l2);

                if (binNum1 == 1)
                    vector1[0] = 1.;
                else
                    {
                    vector1[0] = (ceil(l1 + 0.00001) - l1) / binLen1;
                    for (int t = 1; t< binNum1 - 1; t++)
                        {vector1[t] = 1. / binLen1;}
                    vector1[binNum1 - 1] = (h1 - floor(h1)) / binLen1;
                    }

                if (binNum2 == 1) vector2[0] = 1.;

                else
                    {
                    vector2[0] = (ceil(l2 + 0.0001) - l2) / binLen2;
                    for (int t = 1; t< binNum2 - 1; t++)
                        {vector2[t] = 1. / binLen2;}
                    vector2[binNum2 - 1] = (h2 - floor(h2)) / binLen2;
                    }

                for (int i = 0; i< binNum1; i++)
                    {
                    for (int j = 0; j < binNum2; j++)
                        {
                        heatmap[(b1 + i) * heatmapSize +  b2 + j] += vector1[i] * vector2[j];
                        }
                    }
                }
        """
        weave.inline(code,
                     ['low1', "high1", "low2", "high2",
                       "N", "heatmap", "maxBinSpawn",
                      "heatmapSize",
                       ],
                     extra_compile_args=['-march=native  -O3 '],
                     support_code=r"""
                    #include <stdio.h>
                    #include <math.h>""")

        counts = heatmap
        for i in xrange(len(counts)):
            counts[i, i:] += counts[i:, i]
            counts[i:, i] = counts[i, i:]
            diag = np.diag(counts)
        if countDiagonalReads.lower() == "once":
            fillDiagonal(counts, diag / 2)
        elif countDiagonalReads.lower() == "twice":
            pass
        else:
            raise ValueError("Bad value for countDiagonalReads")
        return counts

    def saveHiResHeatmapWithOverlaps(self, filename, resolution, countDiagonalReads="Twice", maxBinSpawn=10, chromosomes="all"):
        """Creates within-chromosome heatmaps at very high resolution,
        assigning each fragment to all the bins it overlaps with,
        proportional to the area of overlaps.

        Parameters
        ----------
        resolution : int or str
            Resolution of a heatmap.
        countDiagonalReads : "once" or "twice"
            How many times to count reads in the diagonal bin
        maxBinSpawn : int, optional, not more than 10
            Discard read if it spawns more than maxBinSpawn bins

        """
        from scipy import weave

        tosave = h5dict(filename)
        self.genome.setResolution(resolution)
        if chromosomes == "all":
            chromosomes = range(self.genome.chrmCount)
        for chrom in chromosomes:
            print "Saving chromosome {0}".format(chrom)
            mask = (self.chrms1 == chrom) * (self.chrms2 == chrom)

            if mask.sum() == 0:
                continue

            low1 = (self.mids1[mask] - self.fraglens1[mask] / 2) / float(resolution)

            high1 = (self.mids1[mask] + self.fraglens1[mask] / 2) / float(resolution)

            low2 = (self.mids2[mask] - self.fraglens2[mask] / 2) / float(resolution)

            high2 = (self.mids2[mask] + self.fraglens2[mask] / 2) / float(resolution)

            del mask

            N = len(low1)

            heatmapSize = int(self.genome.chrmLensBin[chrom])

            heatmap = np.zeros((heatmapSize, heatmapSize),
                               dtype="float64", order="C")


            code = """
            #line 1045 "fragmentHiC.py"
            double vector1[100];
            double vector2[100];

            for (int readNum = 0;  readNum < N; readNum++)
            {
                for (int i=0; i<10; i++)
                {
                    vector1[i] = 0;
                    vector2[i] = 0;
                }

                double l1 = low1[readNum];
                double l2 = low2[readNum];
                double h1 = high1[readNum];
                double h2 = high2[readNum];


                if ((h1 - l1) > maxBinSpawn) continue;
                if ((h2 - l2) > maxBinSpawn) continue;

                int binNum1 = ceil(h1) - floor(l1);
                int binNum2 = ceil(h2) - floor(l2);
                double binLen1 = h1 - l1;
                double binLen2 = h2 - l2;

                int b1 = floor(l1);
                int b2 = floor(l2);

                if (binNum1 == 1)
                    vector1[0] = 1.;
                else
                    {
                    vector1[0] = (ceil(l1 + 0.00001) - l1) / binLen1;
                    for (int t = 1; t< binNum1 - 1; t++)
                        {vector1[t] = 1. / binLen1;}
                    vector1[binNum1 - 1] = (h1 - floor(h1)) / binLen1;
                    }

                if (binNum2 == 1) vector2[0] = 1.;

                else
                    {
                    vector2[0] = (ceil(l2 + 0.0001) - l2) / binLen2;
                    for (int t = 1; t< binNum2 - 1; t++)
                        {vector2[t] = 1. / binLen2;}
                    vector2[binNum2 - 1] = (h2 - floor(h2)) / binLen2;
                    }

                for (int i = 0; i< binNum1; i++)
                    {
                    for (int j = 0; j < binNum2; j++)
                        {
                        heatmap[(b1 + i) * heatmapSize +  b2 + j] += vector1[i] * vector2[j];
                        }
                    }
                }
        """
            weave.inline(code,
                         ['low1', "high1", "low2", "high2",
                           "N", "heatmap", "maxBinSpawn",
                          "heatmapSize",
                           ],
                         extra_compile_args=['-march=native  -O3 '],
                         support_code=r"""
                        #include <stdio.h>
                        #include <math.h>""")
            del high1, low1, high2, low2


            for i in xrange(len(heatmap)):
                heatmap[i, i:] += heatmap[i:, i]
                heatmap[i:, i] = heatmap[i, i:]
            if countDiagonalReads.lower() == "once":
                diag = np.diag(heatmap).copy()
                fillDiagonal(heatmap, diag / 2)
                del diag
            elif countDiagonalReads.lower() == "twice":
                pass
            else:
                raise ValueError("Bad value for countDiagonalReads")
            tosave["{0} {0}".format(chrom)] = heatmap
            tosave.flush()
            del heatmap
            weave.inline("")  # to release all buffers of weave.inline
            import gc
            gc.collect()
        print "----> By chromosome Heatmap saved to '{0}' at {1} resolution".format(filename, resolution)



    def buildSinglesCoverage(self, resolution):
        """deprecated"""
        return np.zeros(self.genome.numBins)

    def buildFragmetCoverage(self, resolution):
        """creates restriction site density vector (visible sites only)
        in accordance with the 'genome' class"""
        self._buildFragments()
        self.genome.setResolution(resolution)
        chroms = self.ufragments / self.fragIDmult
        positions = self.ufragments % self.fragIDmult
        label = self.genome.chrmStartsBinCont[chroms] + positions / resolution
        counts = np.bincount(label, minlength=self.genome.numBins)
        if len(counts) > self.genome.numBins:
            print "Extra chromosomes found in the data; ignoring"
            counts = counts[:self.genome.numBins]
        assert len(counts) == self.genome.numBins
        return counts

    def fragmentFilter(self, fragments):
        """keeps only reads that originate from fragments in 'fragments'
        variable, for DS - on both sides

        Parameters
        ----------
        fragments : numpy.array of fragment IDs or bools
            List of fragments to keep, or their indexes in self.ufragments
        """
        if fragments.dtype == np.bool:
            self._buildFragments()
            fragments = self.ufragments[fragments]
        m1 = arrayInArray(self.fragids1, fragments)
        m2 = arrayInArray(self.fragids2, fragments)
        mask = np.logical_and(m1, m2)
        self.maskFilter(mask)

    def maskFilter(self, mask):
        """keeps only reads designated by mask

        Parameters
        ----------
        mask : array of bools
            Indexes of reads to keep
        """
        #Uses 16 bytes per read

        length = 0
        ms = mask.sum()
        assert mask.dtype == np.bool
        self.N = ms
        self.DSnum = self.N
        if hasattr(self, "ufragments"):
            del self.ufragmentlen, self.ufragments
        for name in self.vectors:
            data = self._getData(name)
            ld = len(data)
            if length == 0:
                length = ld
            else:
                if ld != length:
                    self.delete()
            newdata = fasterBooleanIndexing(data, mask, outLen=ms,
                                        bounds=False)  # see mirnylib.numutils
            del data
            self._setData(name, newdata)
            del newdata
        del mask
        self.rebuildFragments()

    def rebuildFragments(self):
        "recreates a set of fragments - runs when reads have changed"
        try:
            past = len(self.ufragments)
        except:
            past = 0
        assert len(self.fragids2) == self.N
        ufragids1, ufragids1ind = chunkedUnique(
            self.fragids1,
            return_index=True, chunksize=self.chunksize)
        ufragids2, ufragids2ind = chunkedUnique(
            self.fragids2,
            return_index=True, chunksize=self.chunksize)

        #Funding unique fragments and unique fragment IDs
        ufragment1len = self.fraglens1[ufragids1ind]
        ufragment2len = self.fraglens2[ufragids2ind]

        uall = np.r_[ufragids1, ufragids2]
        ulen = np.r_[ufragment1len, ufragment2len]

        self.ufragments, ind = np.unique(uall, True)
        self.ufragmentlen = ulen[ind]
        self._dumpMetadata()

    def filterExtreme(self, cutH=0.005, cutL=0):
        """removes fragments with most and/or least # counts

        Parameters
        ----------
        cutH : float, 0<=cutH < 1, optional
            Fraction of the most-counts fragments to be removed
        cutL : float, 0<=cutL<1, optional
            Fraction of the least-counts fragments to be removed
        """
        self._buildFragments()
        print "----->Extreme fragments filter: remove top %lf, "\
        "bottom %lf fragments" % (cutH, cutL)

        s = self.fragmentSum()
        ss = np.sort(s)

        valueL, valueH = np.percentile(ss, [100. * cutL, 100 * (1. - cutH)])
        news = (s >= valueL) * (s <= valueH)
        N1 = self.N
        self.fragmentFilter(self.ufragments[news])
        self.metadata["350_removedFromExtremeFragments"] = N1 - self.N
        self._dumpMetadata()

        print "     #Top fragments are: ", ss[-10:]
        print "     # Cutoff for low # counts is (counts): ", valueL,
        print  "; cutoff for large # counts is: ", valueH, "\n"

    def filterLarge(self, cutlarge=100000, cutsmall=100):
        """removes very large and small fragments

        Parameters
        ----------
        cutlarge : int
            remove fragments larger than it
        cutsmall : int
            remove fragments smaller than it
        """
        self._buildFragments()
        print "----->Small/large fragments filter: keep strictly less"\
        "than %d,strictly more than %d bp" % (cutlarge, cutsmall)
        p = (self.ufragmentlen < (cutlarge)) * (self.ufragmentlen > cutsmall)
        N1 = self.N
        self.fragmentFilter(self.ufragments[p])
        N2 = self.N
        self.metadata["340_removedLargeSmallFragments"] = N1 - N2
        self._dumpMetadata()

    def filterRsiteStart(self, offset=5):
        """Removes reads that start within x bp near rsite

        Parameters
        ----------

        offset : int
            Number of bp to exclude next to rsite, not including offset

        """

        #TODO:(MI) fix this so that it agrees with the definition.

        print "----->Semi-dangling end filter: remove guys who start %d"\
        " bp near the rsite" % offset

        expression = "mask = (np.abs(dists1 - fraglens1) >= offset) * "\
        "((np.abs(dists2 - fraglens2) >= offset) )"
        mask = self.evaluate(expression,
                             internalVariables=["dists1", "fraglens1",
                                                "dists2", "fraglens2"],
                             constants={"offset": offset, "np": np},
                             outVariable=("mask", np.zeros(self.N, bool)))
        self.metadata["310_startNearRsiteRemoved"] = len(mask) - mask.sum()
        self.maskFilter(mask)

    def filterDuplicates(self):
        "removes duplicate molecules in DS reads"
        "TODO: rewrite it when Anton allows direct creation of Hi-C datasets"
        #Uses a lot!
        print "----->Filtering duplicates in DS reads: "

        Nds = self.N

        # an array to determine unique rows. Eats 16 bytes per DS record
        dups = np.zeros((Nds, 2), dtype="int64", order="C")

        dups[:, 0] = self.chrms1
        dups[:, 0] *= self.fragIDmult
        dups[:, 0] += self.cuts1
        dups[:, 1] = self.chrms2
        dups[:, 1] *= self.fragIDmult
        dups[:, 1] += self.cuts2
        dups.sort(axis=1)
        dups.shape = (Nds * 2)
        strings = dups.view("|S16")
            #Converting two indices to a single string to run unique
        uids = uniqueIndex(strings)
        del strings, dups
        stay = np.zeros(Nds, bool)
        stay[uids] = True  # indexes of unique DS elements
        del uids
        uflen = len(self.ufragments)
        self.metadata["320_duplicatesRemoved"] = len(stay) - stay.sum()
        self.maskFilter(stay)
        assert len(self.ufragments) == uflen  # self-check

    def filterByCisToTotal(self, cutH=0.0, cutL=0.01):
        """Remove fragments with too low or too high cis-to-total ratio.
        Parameters
        ----------
        cutH : float, 0<=cutH < 1, optional
            Fraction of the fragments with largest cis-to-total ratio
            to be removed.
        cutL : float, 0<=cutL<1, optional
            Fraction of the fragments with lowest cis-to-total ratio
            to be removed.
        """

        if 'rfragAbsIdxs1' not in self.vectors:
            raise Exception('Run setRfragAbsIdxs() first!')

        concRfragAbsIdxs = np.r_[self.rfragAbsIdxs1, self.rfragAbsIdxs2]
        concCis = np.r_[self.chrms1 == self.chrms2, self.chrms1 == self.chrms2]

        cis = np.bincount(concRfragAbsIdxs[concCis])
        total = np.bincount(concRfragAbsIdxs)
        cistototal = np.nan_to_num(cis / total.astype('float'))
        numEmptyFrags = (cistototal == 0).sum()

        cutLFrags = int(np.ceil((len(cistototal) - numEmptyFrags) * cutL))
        cutHFrags = int(np.ceil((len(cistototal) - numEmptyFrags) * cutH))

        sortedCistotot = np.sort(cistototal)

        lCutoff = sortedCistotot[cutLFrags + numEmptyFrags]
        hCutoff = sortedCistotot[len(cistototal) - 1 - cutHFrags]

        fragsToFilter = np.where((cistototal < lCutoff) + (cistototal > hCutoff))[0]
        print ('Keep fragments with cis-to-total ratio in range ({0},{1}), '
               'discard {2} fragments').format(lCutoff, hCutoff, cutLFrags + cutHFrags)

        mask = (arrayInArray(self.rfragAbsIdxs1, fragsToFilter) +
                arrayInArray(self.rfragAbsIdxs2, fragsToFilter))

        self.metadata["330_removedByCisToTotal"] = mask.sum()

        self.maskFilter(-mask)

    def filterTooClose(self, minRsitesDist=2):
        """
        Remove fragment pairs separated by less then `minRsitesDist`
        restriction sites within the same chromosome.
        """

        if 'rfragAbsIdxs1' not in self.vectors:
            raise Exception('Run setRfragAbsIdxs() first!')

        mask = (
            (np.abs(self.rfragAbsIdxs1 - self.rfragAbsIdxs2) < minRsitesDist)
            * (self.chrms1 == self.chrms2))
        self.metadata["360_closeFragmentsRemoved"] = mask.sum()

        self.maskFilter(-mask)

    def writeFilteringStats(self):
        self.metadata["400_readsAfterFiltering"] = self.N
        sameChrom = self.chrms1 == self.chrms2
        self.metadata["401_cisReads"] = sameChrom.sum()
        self.metadata["402_transReads"] = self.N - sameChrom.sum()
        self._dumpMetadata()

    def fragmentSum(self, fragments=None, strands="both", useWeights=False):
        """returns sum of all counts for a set or subset of fragments


        Parameters
        ----------
        fragments : list of fragment IDs, optional
            Use only this fragments. By default all fragments are used
        strands : 1,2 or "both" (default)
            Use only first or second side of the read
            (first has SS, second - doesn't)
        useWeights : bool, optional
            If set to True, will give a fragment sum with weights adjusted for iterative correction.
        """
        #Uses 16 bytes per read
        self._buildFragments()
        if fragments is None:
            fragments = self.ufragments

        if not useWeights:
            if strands == "both":
                return sumByArray(self.fragids1, fragments) + \
                    sumByArray(self.fragids2, fragments)

            if strands == 1:
                return sumByArray(self.fragids1, fragments)
            if strands == 2:
                return sumByArray(self.fragids2, fragments)
        else:
            if strands == "both":
                self.fragmentWeights = 1. * self.fragmentWeights
                pass1 = 1. / self.fragmentWeights[arraySearch(self.ufragments, self.fragids1)]
                pass1 /= self.fragmentWeights[arraySearch(self.ufragments, self.fragids2)]
                return arraySumByArray(self.fragids1, fragments, pass1) + arraySumByArray(self.fragids2, fragments, pass1)
            else:
                raise NotImplementedError("Sorry")

    def iterativeCorrectionFromMax(self, minimumCount=50, precision=0.01):
        self.fragmentWeights = 1. * self.fragmentSum()
        self.fragmentFilter(self.fragmentWeights > minimumCount)
        self.fragmentWeights = 1. * self.fragmentSum()

        while True:
            newSum = 1. * self.fragmentSum(useWeights=True)
            maxDev = np.max(np.abs(newSum - newSum.mean())) / newSum.mean()
            print maxDev

            self.fragmentWeights *= (newSum / newSum.mean())
            if maxDev < precision:
                return

    def printStats(self):
        self.printMetadata()

    def save(self, filename):
        "Saves dataset to filename, does not change the working file."
        if self.filename == filename:
            raise StandardError("Cannot save to the working file")
        newh5dict = h5dict(filename, mode='w')
        for name in self.vectors.keys():
            newh5dict[name] = self.h5dict[name]
        newh5dict["metadata"] = self.metadata
        print "----> Data saved to file %s" % (filename,)

    def load(self, filename, buildFragments=True):
        "Loads dataset from file to working file; check for inconsistency"
        otherh5dict = h5dict(filename, 'r')
        if "metadata" in otherh5dict:
            self.metadata = otherh5dict["metadata"]
        else:
            print otherh5dict.keys()
            warnings.warn("Metadata not found!!!")

        length = 0
        for name in self.vectors:
            data = otherh5dict[name]
            ld = len(data)
            if length == 0:
                length = ld
            else:
                if ld != length:
                    print("---->!!!!!File %s contains inconsistend data<----" %
                          filename)
                    self.exitProgram("----> Sorry...")

            self._setData(name, data)
        print "---->Loaded data from file %s, contains %d reads" % (
            filename, length)
        self.DSnum = self.N = length


        if buildFragments == True:
            self.rebuildFragments()

    def saveHeatmap(self, filename, resolution=1000000,
                    countDiagonalReads="Once",
                    useWeights=False,
                    useFragmentOverlap=False, maxBinSpawn=10):
        """
        Saves heatmap to filename at given resolution.
        For small genomes where number of fragments per bin is small,
        please set useFragmentOverlap to True.
        This will assign each fragment to all bins over which the fragment
        spawns.

        Parameters
        ----------
        filename : str
            Filename of the output h5dict
        resolution : int or str
            Resolution of a heatmap. May be an int or 'fragment' for
            restriction fragment resolution.
        countDiagonalReads : "once" or "twice"
            How many times to count reads in the diagonal bin
        useWeights : bool
            If True, then take weights from 'weights' variable. False by default.
            If using iterativeCorrectionFromMax (fragment-level IC), use weights.
        useFragmentOverlap : bool (optional)
            Set this to true if you have few fragments per bin (bin size <20kb for HindIII)
            It will consume more RAM and be slower.
        """

        try:
            os.remove(filename)
        except:
            pass

        tosave = h5dict(path=filename, mode="w")
        if not useFragmentOverlap:
            heatmap = self.buildAllHeatmap(resolution, countDiagonalReads, useWeights)
        else:
            heatmap = self.buildHeatmapWithOverlapCpp(resolution, countDiagonalReads, maxBinSpawn)

        tosave["heatmap"] = heatmap
        del heatmap
        if resolution != 'fragment':
            singles = self.buildSinglesCoverage(resolution)
            frags = self.buildFragmetCoverage(resolution)
            chromosomeStarts = np.array(self.genome.chrmStartsBinCont)
            numBins = self.genome.numBins
        else:
            singles, frags = [], []
            chromosomeStarts = np.array(self.genome.chrmStartsRfragCont)
            numBins = self.genome.numRfrags
        tosave["resolution"] = resolution
        tosave["singles"] = singles
        tosave["frags"] = frags
        tosave["genomeBinNum"] = numBins
        tosave["genomeIdxToLabel"] = self.genome.idx2label
        tosave["chromosomeStarts"] = chromosomeStarts
        print "----> Heatmap saved to '{0}' at {1} resolution".format(
            filename, resolution)

    def saveByChromosomeHeatmap(self, filename, resolution=10000,
                                includeTrans=True,
                                countDiagonalReads="Once"):
        """
        Saves chromosome by chromosome heatmaps to h5dict.
        This method is not as memory demanding as saving allxall heatmap.

        Keys of the h5dict are of the format ["1 14"],
        where chromosomes are zero-based,
        and there is one space between numbers.

        .. warning :: Chromosome numbers are always zero-based.
        Only "chr3" labels are one-based in this package.

        Parameters
        ----------

        filename : str
            Filename of the h5dict with the output
        resolution : int
            Resolution to save heatmaps
        includeTrans : bool, optional
            Build inter-chromosomal heatmaps (default: False)
        countDiagonalReads : "once" or "twice"
            How many times to count reads in the diagonal bin

        """
        if countDiagonalReads.lower() not in ["once", "twice"]:
            raise ValueError("Bad value for countDiagonalReads")
        self.genome.setResolution(resolution)
        pos1 = self.evaluate("a = np.array(mids1 / {res}, dtype = 'int32')"
                             .format(res=resolution), "mids1")
        pos2 = self.evaluate("a = np.array(mids2 / {res}, dtype = 'int32')"
                             .format(res=resolution), "mids2")
        chr1 = self.chrms1
        chr2 = self.chrms2
        #DS = self.DS  # 13 bytes per read up to now, 16 total
        mydict = h5dict(filename)

        for chrom in xrange(self.genome.chrmCount):
            if includeTrans == True:
                mask = ((chr1 == chrom) + (chr2 == chrom))
            else:
                mask = ((chr1 == chrom) * (chr2 == chrom))
            # Located chromosomes and positions of chromosomes
            c1, c2, p1, p2 = chr1[mask], chr2[mask], pos1[mask], pos2[mask]
            if includeTrans == True:
                #moving different chromosomes to c2
                #c1 == chrom now
                mask = (c2 == chrom) * (c1 != chrom)
                c1[mask], c2[mask], p1[mask], p2[mask] = c2[mask].copy(), c1[
                    mask].copy(), p2[mask].copy(), p1[mask].copy()
                del c1  # ignore c1
                args = np.argsort(c2)
                c2 = c2[args]
                p1 = p1[args]
                p2 = p2[args]

            for chrom2 in xrange(chrom, self.genome.chrmCount):
                if (includeTrans == False) and (chrom2 != chrom):
                    continue
                start = np.searchsorted(c2, chrom2, "left")
                end = np.searchsorted(c2, chrom2, "right")
                cur1 = p1[start:end]
                cur2 = p2[start:end]
                label = np.asarray(cur1, "int64")
                label *= self.genome.chrmLensBin[chrom2]
                label += cur2
                maxLabel = self.genome.chrmLensBin[chrom] * \
                           self.genome.chrmLensBin[chrom2]
                counts = np.bincount(label, minlength=maxLabel)
                assert len(counts) == maxLabel
                mymap = counts.reshape((self.genome.chrmLensBin[chrom], -1))
                if chrom == chrom2:
                    mymap = mymap + mymap.T
                    if countDiagonalReads.lower() == "once":
                        fillDiagonal(mymap, np.diag(mymap).copy() / 2)
                mydict["%d %d" % (chrom, chrom2)] = mymap
        print "----> By chromosome Heatmap saved to '{0}' at {1} resolution".format(filename, resolution)

        return

    def exitProgram(self, a):
        print a
        print "     ----> Bye! :) <----"
        exit()


    def setRfragAbsIdxs(self, rEnzyme):
        """Cache the absolute indices of restriction fragments.
        """

        self.vectors['rfragAbsIdxs1'] = 'int32'
        self.vectors['rfragAbsIdxs2'] = 'int32'

        self.genome.setEnzyme(rEnzyme)
        self.rfragAbsIdxs1 = self.genome.getRfragAbsIdxs(self.fragids1)
        self.rfragAbsIdxs2 = self.genome.getRfragAbsIdxs(self.fragids2)

    def iterativeCorrection(self, numsteps=10, normToLen=False):
        '''
        Perform fragment-based iterative correction of Hi-C data.
        '''
        if 'rfragAbsIdxs1' not in self.vectors:
            raise Exception('Run setRfragAbsIdxs() first!')

        rfragLensConc = np.concatenate(self.genome.rfragLens)
        weights = np.ones(self.N, dtype=np.float32)
        concRfragAbsIdxs = np.r_[self.rfragAbsIdxs1, self.rfragAbsIdxs2]
        concOrigArgs = np.r_[np.arange(0, self.N), np.arange(0, self.N)]
        concArgs = np.argsort(concRfragAbsIdxs)
        concRfragAbsIdxs = concRfragAbsIdxs[concArgs]
        concOrigArgs = concOrigArgs[concArgs]
        fragBorders = np.where(concRfragAbsIdxs[:-1] != concRfragAbsIdxs[1:])[0] + 1
        fragBorders = np.r_[0, fragBorders, 2 * self.N]
        rfragLensLocal = rfragLensConc[concRfragAbsIdxs[fragBorders[:-1]]]
        for _ in range(numsteps):
            for i in range(len(fragBorders) - 1):
                mask = concOrigArgs[fragBorders[i]:fragBorders[i + 1]]
                totWeight = weights[mask].sum()
                if normToLen:
                    weights[mask] *= rfragLensLocal[i] / totWeight
                else:
                    weights[mask] /= totWeight

        self.vectors['weights'] = 'float32'
        self.weights = weights


class HiCStatistics(HiCdataset):
    """a semi-experimental sub-class of a 'HiCdataset' class
     used to do statistics on Hi-C reads
     Please be careful.
     """

    def multiplyForTesting(self, N=100):
        """used for heavy load testing only
        """
        for name in self.vectors:
            print "multipliing", name
            one = self._getData(name)
            blowup = np.hstack(tuple([one] * N))
            if name in ["mids1", "mids2"]:
                blowup += np.random.randint(0, 2 * N, len(blowup))

            self._setData(name, blowup)

    def buildLengthDependencePlot(self, strands="both", normalize=True,
                                  **kwargs):
        """plots dependence of counts on fragment length.
        May do based on one strands only
        "please run  plt.legend & plt.show() after calling this
        for all datasets you want to consider"""
        import matplotlib.pyplot as plt
        self._buildFragments()
        fragmentLength = self.ufragmentlen
        pls = np.sort(fragmentLength)
        N = len(fragmentLength)
        sums = []
        sizes = []
        mysum = self.fragmentSum(None, strands)

        for i in np.arange(0, 0.98, 0.015):
            b1, b2 = pls[i * N], pls[(i + 0.015) * N - 1]
            p = (b1 < fragmentLength) * (b2 > fragmentLength)
            value = np.mean(mysum[p])
            sums.append(value)
            sizes.append(np.mean(fragmentLength[p]))
        sums = np.array(sums)
        if normalize == True:
            sums /= sums.mean()
        plt.plot(sizes, sums, **kwargs)

    def plotScaling(self, fragids1=None, fragids2=None,
                    # IDs of fragments for which to plot scaling.
                    #One can, for example, limit oneself to
                    #only fragments shorter than 1000 bp
                    #Or calculate scaling only between different arms

                    useWeights=False,
                        # use weights associated with fragment length
                    excludeNeighbors=None, enzyme=None,
                        # number of neighboring fragments to exclude.
                        #Enzyme is needed for that!
                    normalize=True, normRange=None,
                        # normalize the final plot to sum to one
                    withinArms=True,
                        #Treat chromosomal arms separately
                    mindist=10000,
                        # Scaling was proved to be unreliable
                        # under 10000 bp for 6-cutter enzymes
                    maxdist=None,

                    #----Calculating scaling within a set of regions only----
                    regions=None,
                    # Array of tuples (chrom, start, end)
                    # for which scaling should be calculated
                    #Note that calculation might be extremely long
                    #(it might be proportional to # of regions for # > 100)

                    appendReadCount=True, **kwargs
                        # Append read count to the plot label
                        # kwargs to be passed to plotting
                    ):  # Sad smiley, because this method
                        # is very painful and complicated
        """plots scaling over, possibly uses subset of fragmetns, or weigts,
        possibly normalizes after plotting

        Plan of scaling calculation:

        1. Subdivide all genome into regions. \n
            a. Different chromosomes \n
            b. Different arms \n
            c. User defined squares/rectangles on a contact map \n
               -(chromosome, start,end) square around the diagonal \n
               -(chr, st1, end1, st2, end2) rectangle \n

        2. Use either all fragments, or only interactions between
        two groups of fragments \n
            e.g. you can calculate how scaling for small fragments is different
            from that for large \n
            It can be possibly used for testing Hi-C protocol issues. \n
            One can see effect of weights by doing this \n

        3. (optional) Calculate correction associated
        with fragment length dependence

        4. Subdivide all possible genomic separation into log-spaced bins

        5. Calculate expected number of fragment pairs within each bin
        (possibly with weights from step 3).

        If exclusion of neighbors is specificed,
        expected number of fragments knows about this

        Parameters
        ----------
        fragids1, fragids2 : np.array of fragment IDs, optional
            Scaling is calculated only for interactions between
            fragids1 and fragids2
            If omitted, all fragments are used
            If boolean array is supplied, it serves as a mask for fragments.
        useWeights : bool, optional
            Use weights calculated from fragment length
        excludeNeighbors : int or None, optional
            If None, all fragment pairs are considered.
            If integer, only fragment pairs separated
            by at least this number of r-fragments are considered.
        enzyme : string ("HindIII","NcoI")
            If excludeNeighbors is used, you have to specify restriction enzyme
        normalize : bool, optional
            Do an overall normalization of the answer, by default True.
        withinArms : bool, optional
            Set to false to use whole chromosomes instead of arms
        mindist, maxdist : int, optional
            Use lengthes from mindist to maxdist
        regions : list of (chrom,start,end) or (ch,st1,end1,st2,end2), optional
            Restrict scaling calculation to only certain squares of the map
        appendReadCount : bool, optional
            Append read count to the plot label
        plot : bool, optional
            If False then do not display the plot. True by default.
        **kwargs :  optional
            All other keyword args are passed to plt.plot

        Returns
        -------
        (bins,probabilities) - values to plot on the scaling plot

        """
        #TODO:(MI) write an ab-initio test for scaling calculation
        import matplotlib.pyplot as plt
        self._buildFragments()
        if excludeNeighbors <= 0:
            excludeNeighbors = None  # Not excluding neighbors

        #use all fragments if they're not specified
        #parse fragment array if it's bool
        if (fragids1 is None) and (fragids2 is None):
            allFragments = True
        else:
            allFragments = False
        if fragids1 is None:
            fragids1 = self.ufragments
        if fragids2 is None:
            fragids2 = self.ufragments
        if fragids1.dtype == np.bool:
            fragids1 = self.ufragments[fragids1]
        if fragids2.dtype == np.bool:
            fragids2 = self.ufragments[fragids2]

        #Calculate regions if not specified
        if regions is None:
            if withinArms == False:
                regions = [(i, 0, self.genome.chrmLens[i])
                    for i in xrange(self.genome.chrmCount)]
            else:
                regions = [(i, 0, self.genome.cntrMids[i])
                    for i in xrange(self.genome.chrmCount)] + \
                    [(i, self.genome.cntrMids[i], self.genome.chrmLens[i])
                    for i in xrange(self.genome.chrmCount)]

        if maxdist is None:
            maxdist = max(
                        max([i[2] - i[1] for i in regions]),
                        # rectangular regions
                        max([abs(i[2] - i[3]) for i in regions if
                             len(i) > 3] + [0]),
                        max([abs(i[1] - i[4]) for i in regions if
                             len(i) > 3] + [0])  # other side
                          )
        # Region to which a read belongs
        regionID = np.zeros(len(self.chrms1), np.int16) - 1
        chr1 = self.chrms1
        chr2 = self.chrms2
        pos1 = self.mids1
        pos2 = self.mids2
        fragRegions1 = np.zeros(len(fragids1), int) - 1
        fragRegions2 = np.zeros(len(fragids2), int) - 1
        fragch1 = fragids1 / self.fragIDmult
        fragch2 = fragids2 / self.fragIDmult
        fragpos1 = fragids1 % self.fragIDmult
        fragpos2 = fragids2 % self.fragIDmult

        for regionNum, region in enumerate(regions):
            if len(region) == 3:
                chrom, start1, end1 = region
                mask = (chr1 == chrom) * (pos1 > start1) * (pos1 < end1) * \
                (chr2 == chrom) * (pos2 > start1) * (pos2 < end1)
                regionID[mask] = regionNum
                mask1 = (fragch1 == chrom) * (fragpos1 >
                    start1) * (fragpos1 < end1)
                mask2 = (fragch2 == chrom) * (fragpos2 >
                    start1) * (fragpos2 < end1)
                fragRegions1[mask1] = regionNum
                fragRegions2[mask2] = regionNum

            if len(region) == 5:
                chrom, start1, end1, start2, end2 = region

                mask1 = (chr1 == chrom) * (chr2 == chrom) * (pos1 > start1) * \
                (pos1 < end1) * (pos2 > start2) * (pos2 < end2)

                mask2 = (chr1 == chrom) * (chr2 == chrom) * (pos1 > start2) * \
                (pos1 < end2) * (pos2 > start1) * (pos2 < end1)

                mask = mask1 + mask2
                regionID[mask] = regionNum
                mask1 = (fragch1 == chrom) * (
                    (fragpos1 > start1) * (fragpos1 < end1)
                    + (fragpos1 > start2) * (fragpos1 < end2))
                mask2 = (fragch2 == chrom) * (
                    (fragpos2 > start2) * (fragpos2 < end2)
                    + (fragpos2 > start1) * (fragpos2 < end1))
                fragRegions1[mask1] = regionNum
                fragRegions2[mask2] = regionNum
        del chr1, chr2, pos1, pos2

        bins = np.array(
            numutils.logbins(mindist, maxdist, 1.12), float) + 0.1  # bins of lengths
        numBins = len(bins) - 1  # number of bins

        #Keeping reads for fragments in use
        # Consider only double-sided fragment pairs.
        validFragPairs = (regionID >= 0)
        if allFragments == False:
            # Filter the dataset so it has only the specified fragments.
            p11 = arrayInArray(self.fragids1, fragids1)
            p12 = arrayInArray(self.fragids1, fragids2)
            p21 = arrayInArray(self.fragids2, fragids1)
            p22 = arrayInArray(self.fragids2, fragids2)
            validFragPairs *= ((p11 * p22) + (p12 * p21))

        # Consider pairs of fragments from the same region.

        # Keep only --> -->  or <-- <-- pairs, discard --> <-- and <-- -->

        validFragPairs *= (self.strands1 == self.strands2)

        # Keep only fragment pairs more than excludeNeighbors fragments apart.
        if excludeNeighbors is not None:
            if enzyme is None:
                raise ValueError("Please specify enzyme if you're"
                                 " excluding Neighbors")
            distsInFrags = self.genome.getFragmentDistance(
                self.fragids1, self.fragids2, enzyme)

            validFragPairs *= distsInFrags > excludeNeighbors

        distances = np.sort(self.distances[validFragPairs])

        "calculating fragments lengths for exclusions to expected # of counts"
        #sorted fragment IDs and lengthes
        args = np.argsort(self.ufragments)
        usort = self.ufragments[args]

        if useWeights == True:  # calculating weights if needed
            try:
                self.fragmentWeights
            except:
                self.calculateFragmentWeights()
            uweights = self.fragmentWeights[args]  # weights for sorted fragment IDs
            weights1 = uweights[np.searchsorted(usort, fragids1)]
            weights2 = uweights[np.searchsorted(usort, fragids2)
                ]  # weghts for fragment IDs under  consideration

        binBegs, binEnds = bins[:-1], bins[1:]

        numExpFrags = np.zeros(numBins)  # count of reads in each min
        fragpos1 = fragids1 % self.fragIDmult
        fragpos2 = fragids2 % self.fragIDmult

        for regionNumber, region in enumerate(regions):
            print region

            # filtering fragments that correspond to current region
            mask1 = np.nonzero(fragRegions1 == regionNumber)[0]
            mask2 = np.nonzero(fragRegions2 == regionNumber)[0]

            if (len(mask1) == 0) or (len(mask2) == 0):
                continue
            bp1, bp2 = fragpos1[mask1], fragpos2[mask2]
                #positions of fragments on chromosome

            p2arg = np.argsort(bp2)
            p2 = bp2[p2arg]  # sorted positions on the second fragment

            if excludeNeighbors is not None:
                "calculating excluded fragments (neighbors) and their weights"\
                " to subtract them later"
                excFrag1, excFrag2 = self.genome.getPairsLessThanDistance(
                    fragids1[mask1], fragids2[mask2], excludeNeighbors, enzyme)
                excDists = np.abs(excFrag2 - excFrag1)
                    #distances between excluded fragment pairs
                if useWeights == True:
                    correctionWeights = weights1[numutils.arraySearch(
                        fragids1, excFrag1)]

                    # weights for excluded fragment pairs
                    correctionWeights = correctionWeights * weights2[
                                numutils.arraySearch(fragids2, excFrag2)]
            if useWeights == True:
                w1, w2 = weights1[mask1], weights2[mask2]
                sw2 = np.r_[0, np.cumsum(w2[p2arg])]
                    #cumsum for sorted weights on 2 strand

            for minDist, maxDist, binIndex in zip(binBegs, binEnds, range(numBins)):
                "Now calculating actual number of fragment pairs for a "\
                "length-bin, or weight of all these pairs"

                # For each first fragment in a pair, calculate total # of
                # restriction fragments in the genome lying downstream within
                # the bin.
                val1 = np.searchsorted(p2, bp1 - maxDist)
                val2 = np.searchsorted(p2, bp1 - minDist)
                if useWeights == False:
                    curcount = np.sum(np.abs(val1 - val2))  # just # of fragments
                else:
                    # (difference in cumsum of weights) * my weight
                    curcount = np.sum(w1 * np.abs(sw2[val1] - sw2[val2]))

                # Repeat the procedure for the fragments lying upstream.
                val1 = np.searchsorted(p2, bp1 + maxDist)
                val2 = np.searchsorted(p2, bp1 + minDist)
                if useWeights == False:
                    curcount += np.sum(np.abs(val1 - val2))
                else:
                    curcount += np.sum(w1 * np.abs(sw2[val1] - sw2[val2]))

                # now modifying expected count because of excluded fragments
                if excludeNeighbors is not None:
                    if useWeights == False:
                        ignore = ((excDists > minDist) *
                            (excDists < maxDist)).sum()
                    else:
                        ignore = (correctionWeights[((excDists > minDist) * \
                                                (excDists < maxDist))]).sum()

                    if (ignore >= curcount) and (ignore != 0):
                        if ignore < curcount * 1.0001:
                            curcount = ignore = 0
                        else:
                            print "error found", "minDist:", minDist
                            print "  curcount:", curcount, "  ignore:", ignore
                    else:  # Everything is all right
                        curcount -= ignore
                numExpFrags[binIndex] += curcount

        values = []
        rawValues = []
        binBegs, binEnds = bins[:-1], bins[1:]
        binMids = 0.5 * (binBegs + binEnds).astype(float)
        binLens = binEnds - binBegs

        for i in xrange(len(bins) - 1):  # Dividing observed by expected
            first, last = tuple(np.searchsorted(distances, [binBegs[i], binEnds[i]]))
            mycounts = last - first
            values.append(mycounts / float(numExpFrags[i]))
            rawValues.append(mycounts)

        values = np.array(values)

        if normalize == True:
            if normRange is None:
                values /= np.sum(
                    1. * (binLens * values)[
                        np.logical_not(
                            np.isnan(binMids * values))])
            else:
                values /= np.sum(
                    1. * (binLens * values)[
                        np.logical_not(
                            np.isnan(binMids * values))
                            * (binMids > normRange[0])
                            * (binMids < normRange[1])])

        do_plot = kwargs.pop('plot', True)
        if do_plot:
            if appendReadCount == True:
                if "label" in kwargs.keys():
                    kwargs["label"] = kwargs["label"] + \
                        ", %d reads" % len(distances)
            plt.plot(binMids, values, **kwargs)
        return (binMids, values)

    def plotRsiteStartDistribution(self, useSSReadsOnly="deprecated",
                                   offset=5, length=200):
        """
        run plt.show() after this function.
        """
        import matplotlib.pyplot as plt
        dists1 = self.fraglens1 - np.array(self.dists1, dtype="int32")
        dists2 = self.fraglens2 - np.array(self.dists2, dtype="int32")
        m = min(dists1.min(), dists2.min())
        if offset < -m:
            offset = -m
            print "minimum negative distance is %d, larger than offset;"\
            " offset set to %d" % (m, -m)
        dists1 += offset
        dists2 += offset
        myrange = np.arange(-offset, length - offset)

        plt.subplot(141)
        plt.title("strands1, side 1")
        plt.plot(myrange, np.bincount(
            5 + dists1[self.strands1 == True])[:length])
        plt.subplot(142)
        plt.title("strands1, side 2")
        plt.plot(myrange, np.bincount(
            dists2[self.strands1 == True])[:length])

        plt.subplot(143)
        plt.title("strands2, side 1")
        plt.plot(myrange, np.bincount(
            dists1[self.strands1 == False])[:length])
        plt.subplot(144)
        plt.title("strands2, side 2")
        plt.plot(myrange, np.bincount(
            dists2[self.strands1 == False])[:length])
